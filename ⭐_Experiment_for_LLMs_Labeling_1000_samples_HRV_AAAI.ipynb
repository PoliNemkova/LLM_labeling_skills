{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PoliNemkova/LLM_labeling_skills/blob/main/%E2%AD%90_Experiment_for_LLMs_Labeling_1000_samples_HRV_AAAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7kdo2i3vxIo"
      },
      "source": [
        "### Housekeeping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmxwGacczZg8",
        "outputId": "fdc96fad-dce6-49f1-9e4e-226aee93c659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\n",
            "Requirement already satisfied: openai==0.27.0 in /usr/local/lib/python3.10/dist-packages (0.27.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.27.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.0) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.27.0) (3.11.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.0) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.27.0) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install ipython-autotime\n",
        "!pip install openai==0.27.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pljxAW1snmXm",
        "outputId": "2e3cb2d5-121f-4d24-bbdf-73a56ad6653d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "time: 504 ms (started: 2024-12-09 19:39:14 +00:00)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%load_ext autotime\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT-SxcAMhujh",
        "outputId": "e1aeb8ed-68b3-4f4a-dac3-7ff881bcab44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6.48 ms (started: 2024-12-09 23:43:12 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# API access keys\n",
        "\n",
        "api_keys = pd.read_csv('/content/gdrive/MyDrive/Computing/api_keys.csv')\n",
        "api_keys = pd.DataFrame(api_keys)\n",
        "openai_api_key = api_keys['key'][api_keys['api'] =='openai'].iloc[0]\n",
        "claude_api_key = api_keys['key'][api_keys['api'] =='claude'].iloc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV-UbhPDufgH"
      },
      "source": [
        "##HRV TESTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HunLugq7u0F4"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfnV6mA3STaC",
        "outputId": "c3c7768f-c554-486b-b083-0672e59f5b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 38.5 ms (started: 2024-12-09 23:38:19 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# loading annotated dataset\n",
        "\n",
        "import pandas as pd\n",
        "human_annotations = pd.read_csv('/content/gdrive/MyDrive/HRV NLP Project X/Data/Data [January 2024]/Annotations/all_adjudicated.csv')\n",
        "\n",
        "# declare how many samples to use\n",
        "n = 1000\n",
        "#n = len(human_annotations)\n",
        "human_annotations = human_annotations[:n]\n",
        "human_labels = human_annotations['Adjudicated HRV label'].tolist()\n",
        "data = human_annotations['Post'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnG8hrZ4BBnM",
        "outputId": "a11a16b0-1e1b-4a05-e352-d687e7e13902"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.77 ms (started: 2024-12-09 18:51:22 +00:00)\n"
          ]
        }
      ],
      "source": [
        "len(human_annotations)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get language breakdow in the sample set\n",
        "\n",
        "\n",
        "!pip install langdetect\n",
        "\n",
        "from langdetect import detect, DetectorFactory\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure consistent results by setting a seed\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "data = data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Detect language for each string\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return 'unknown'\n",
        "\n",
        "human_annotations['language'] = human_annotations['Post'].apply(detect_language)\n",
        "\n",
        "# Get breakdown\n",
        "language_counts = human_annotations['language'].value_counts()\n",
        "\n",
        "# Display results\n",
        "print(language_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSkEnJuMZyF7",
        "outputId": "eca4e4d0-b8c4-4d87-c20b-60dad1dfba93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.5/981.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=065ca705c424c0cd4c4c0d990c6b58bc3f1846fd1902cd1dcbd4bfae60ec17d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "language\n",
            "ru    966\n",
            "uk     31\n",
            "bg      2\n",
            "en      1\n",
            "Name: count, dtype: int64\n",
            "time: 9.46 s (started: 2024-12-10 01:40:00 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW9TNo-QCXf3",
        "outputId": "14b10298-13d3-4d6e-d497-5683be7a77fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa: 0.6309607855721713\n",
            "time: 1.85 s (started: 2024-12-09 18:51:22 +00:00)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "kappa = cohen_kappa_score(human_annotations['HRV 1'].to_list(), human_annotations['HRV 2'].to_list())\n",
        "print(f\"Cohen's Kappa: {kappa}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxqgO6EvFlTd",
        "outputId": "23a0dd96-6d6a-4e2a-dc1b-bdbe90f14047"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "184"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6 ms (started: 2024-12-09 18:51:24 +00:00)\n"
          ]
        }
      ],
      "source": [
        "human_agreement = human_annotations[human_annotations['HRV 1']==human_annotations['HRV 2']]\n",
        "human_disagreement = human_annotations[human_annotations['HRV 1']!=human_annotations['HRV 2']]\n",
        "human_annotations = pd.concat([human_agreement, human_disagreement], ignore_index=True) # I want to have forst 85 row with perfect agreement first, and then 15 with disagreement\n",
        "len(human_disagreement)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "hDDHNGFEDetU",
        "outputId": "edf0fda0-f46f-48eb-eab0-eec88ab79e87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adjudicated HRV label\n",
              "Yes    517\n",
              "No     483\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Adjudicated HRV label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Yes</th>\n",
              "      <td>517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>No</th>\n",
              "      <td>483</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6.86 ms (started: 2024-12-09 18:51:24 +00:00)\n"
          ]
        }
      ],
      "source": [
        "human_annotations['Adjudicated HRV label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qoRKJSpoNGQ"
      },
      "source": [
        "###GPT 3.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHjazTPfzwqC"
      },
      "source": [
        "####GPT 3.5 Few Shot, eng prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ccq-V6nhoQBS",
        "outputId": "311a9ce2-d75e-4bf0-91cd-6b96057d9690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  GPT_3.5_Few_Shot_eng_prompt\n",
            "Accuracy: 0.661\n",
            "Precision: 0.6101485148514851\n",
            "Recall: 0.9535783365570599\n",
            "F1 Score: 0.7441509433962264\n",
            "time: 325 ms (started: 2024-12-08 18:23:58 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# GPT 3.5 Few Shot Learning -- ENG instructions, WITH EXAMPLES\n",
        "\n",
        "# had $12.74 on openAI account before running this, ran 5 mins, ended up with $12.48 (cost is $0.26?)\n",
        "import openai\n",
        "\n",
        "name = 'GPT_3.5_Few_Shot_eng_prompt'\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "def label_with_chatgpt(prompt, data):\n",
        "    responses = []\n",
        "    for item in data:\n",
        "        full_prompt = f\"{prompt}\\n\\nData: {item}\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "            max_tokens=50\n",
        "        )\n",
        "        label = response.choices[0].message['content'].strip()\n",
        "        responses.append(label)\n",
        "    return responses\n",
        "\n",
        "\n",
        "### ALL ENGLIGH PROMPT\n",
        "prompt = \"\"\"Instruction: You are a humanitarian state-of-the-art Human Rights Violation (HRV) detection system. Given a social media post, your job is to classify\n",
        "if the post contains HRV mentions. HRV includes (but not limited to): 1) Killing civilians, 2) Destruction of civil objects, 3) Rape, torture, executions. You need to return just either \"Yes\" or \"No\" label.\n",
        "\n",
        "\n",
        "Here are some examples:\n",
        "\n",
        "Example 1\n",
        "\n",
        "Post: In Mykolaiv, after the arrival at the house, there are already five dead…\n",
        "Label: Yes\n",
        "\n",
        "Example 2\n",
        "\n",
        "Post: Russian aviation struck the Neptune pool in Mariupol where there were pregnant women and mothers with children under 3 years old. Orcs fighting against civilians - a special cauldron in hell is prepared for you. Informer | Subscribe\n",
        "Label: Yes\n",
        "\n",
        "Example 3\n",
        "\n",
        "Post: The first episode is played. There will be others. And more. I will express a personal position. I cannot fail to mention this now. The Ukrainian state in its current configuration with a Nazi political regime will pose a constant, direct, and clear threat to Russia. Therefore, in addition to protecting our people and guarding the country's borders, the goal of our future actions, in my opinion, should be the full dismantling of the political regime in Ukraine.\n",
        "Label: No\n",
        "\n",
        "Note that the destruction of military objects (for example, an airbase) is not considered HRV.\n",
        "\n",
        "You need to output label only.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Get labels\n",
        "labels_eng_prompt = label_with_chatgpt(prompt, data)\n",
        "\n",
        "def normalize_label(labels):\n",
        "  out = []\n",
        "  for label in labels:\n",
        "    label = label.lower()  # Normalize to lowercase\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label:  # Check for relevant terms\n",
        "      out.append('Yes')\n",
        "    else:\n",
        "      out.append('No')\n",
        "  return out\n",
        "\n",
        "# Apply the normalization function to the list\n",
        "labels_eng_prompt = normalize_label(labels_eng_prompt)\n",
        "\n",
        "\n",
        "labels_eng_prompt = pd.DataFrame(labels_eng_prompt)\n",
        "\n",
        "\n",
        "\n",
        "# evaluation\n",
        "\n",
        "# encoding\n",
        "#labels_eng_prompt['labels'] = labels_eng_prompt[0].replace({'Yes': 1, 'No': 0})\n",
        "gpt_labels = labels_eng_prompt[0].to_list()\n",
        "\n",
        "\n",
        "\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': gpt_labels,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "y_true = human_labels\n",
        "y_pred = normalize_label(gpt_labels)\n",
        "\n",
        "\n",
        "print(\"Model: \", name)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "recall = recall_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "f1 = f1_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSrUR7uf2dg2"
      },
      "source": [
        "####GPT 3.5 Few Shot, rus prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptLfKnXs2ccZ",
        "outputId": "cdcad19e-edd0-41f3-9d70-0004ad52d81c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.703\n",
            "Precision: 0.6498637602179836\n",
            "Recall: 0.9226305609284333\n",
            "F1 Score: 0.762589928057554\n",
            "Model:  GPT_3.5_Few_Shot_rus_prompt\n",
            "Accuracy: 0.703\n",
            "Precision: 0.6498637602179836\n",
            "Recall: 0.9226305609284333\n",
            "F1 Score: 0.762589928057554\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 6min 1s (started: 2024-12-08 18:27:15 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# GPT 3.5 Few Shot Learning -- ENG instructions, WITH EXAMPLES\n",
        "\n",
        "# started with $12.48, ended with $12.05 ($0.43)\n",
        "import openai\n",
        "\n",
        "name = 'GPT_3.5_Few_Shot_rus_prompt'\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "def label_with_chatgpt(prompt, data):\n",
        "    responses = []\n",
        "    for item in data:\n",
        "        full_prompt = f\"{prompt}\\n\\nData: {item}\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "            max_tokens=50\n",
        "        )\n",
        "        label = response.choices[0].message['content'].strip()\n",
        "        responses.append(label)\n",
        "    return responses\n",
        "\n",
        "\n",
        "### ALL RUSSIAN PROMPT\n",
        "prompt = \"\"\"Инструкция: Вы — современная гуманитарная онлайн система для обнаружения нарушений прав человека. Получив публикацию в социальных сетях, ваша задача — классифицировать,\n",
        "содержит ли она упоминания о нарушениях прав человека. Нарушения включают (но не ограничиваются): 1) Убийство мирных жителей, 2) Уничтожение гражданских объектов, 3) Изнасилование, пытки, казни.\n",
        "Вы должны возвращать только лейбл \"Да\" или \"Нет\".\n",
        "\n",
        "Примеры:\n",
        "\n",
        "Пример 1\n",
        "\n",
        "Пост: В Николаеве, после прилёта в дом, уже пятеро погибших...\n",
        "лейбл: Да\n",
        "\n",
        "Пример 2\n",
        "\n",
        "Пост: Российская авиация нанесла удар по бассейну «Нептун» в Мариуполе, где находились беременные женщины и матери с детьми до 3 лет. Орки, воюющие против мирных жителей, — для вас в аду приготовлен особый котел. Информатор | Подписывайтесь.\n",
        "лейбл: Да\n",
        "\n",
        "Пример 3\n",
        "\n",
        "Пост: Первый эпизод сыгран. Будут другие. И еще. Я выражу личную позицию. Не могу не упомянуть об этом сейчас. Украинское государство в его нынешней конфигурации с нацистским политическим режимом будет представлять постоянную, прямую и явную угрозу для России. Поэтому, помимо защиты нашего народа и охраны границ страны, целью наших будущих действий, по моему мнению, должно стать полное демонтаж политического режима в Украине.\n",
        "лейбл: Нет\n",
        "\n",
        "Учтите, что уничтожение военных объектов (например, авиабазы) не считается нарушением прав человека.\n",
        "\n",
        "Вы должны предоставить только лейбл.\"\"\"\n",
        "\n",
        "\n",
        "#data = data['Post'].to_list()\n",
        "\n",
        "# Get labels\n",
        "labels_eng_prompt = label_with_chatgpt(prompt, data)\n",
        "\n",
        "def normalize_label(labels):\n",
        "  out = []\n",
        "  for label in labels:\n",
        "    label = label.lower()  # Normalize to lowercase\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label or 'Yes' in label:  # Check for relevant terms\n",
        "      out.append('Yes')\n",
        "    else:\n",
        "      out.append('No')\n",
        "  return out\n",
        "\n",
        "# Apply the normalization function to the list\n",
        "labels_eng_prompt = normalize_label(labels_eng_prompt)\n",
        "\n",
        "\n",
        "labels_eng_prompt = pd.DataFrame(labels_eng_prompt)\n",
        "\n",
        "\n",
        "# evaluation\n",
        "\n",
        "# encoding\n",
        "#labels_eng_prompt['labels'] = labels_eng_prompt[0].replace({'Yes': 1, 'No': 0})\n",
        "gpt_labels = labels_eng_prompt[0].to_list()\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_true = human_labels\n",
        "y_pred = gpt_labels\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "print(\"Model: \", name)\n",
        "print(\"Sample size: \", n)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "recall = recall_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "f1 = f1_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': gpt_labels,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQbLXiPJz0b6"
      },
      "source": [
        "####GPT 3.5 One Shot, eng prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_VOSGJJub7z",
        "outputId": "b9a7d7d1-edb2-4f7e-e2fd-fb4dcf519933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  GPT_3.5_One_Shot_eng_prompt\n",
            "Sample size:  1000\n",
            "Accuracy: 0.726\n",
            "Precision: 0.678939617083947\n",
            "Recall: 0.8916827852998066\n",
            "F1 Score: 0.7709030100334449\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 5min 9s (started: 2024-12-08 18:40:05 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# GPT 3.5 One Shot Learning -- ENG instructions, NO EXAMPLES\n",
        "\n",
        "# started with $12.05, ended with $11.90 ($0.15)\n",
        "import openai\n",
        "\n",
        "name = 'GPT_3.5_One_Shot_eng_prompt'\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "def label_with_chatgpt(prompt, data):\n",
        "    responses = []\n",
        "    for item in data:\n",
        "        full_prompt = f\"{prompt}\\n\\nData: {item}\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "            max_tokens=50\n",
        "        )\n",
        "        label = response.choices[0].message['content'].strip()\n",
        "        responses.append(label)\n",
        "    return responses\n",
        "\n",
        "\n",
        "### ALL ENGLIGH PROMPT\n",
        "prompt = \"\"\"Instruction: You are a humanitarian state-of-the-art Human Rights Violation (HRV) detection system. Given a social media post, your job is to classify\n",
        "if the post contains HRV mentions. HRV includes (but not limited to): 1) Killing civilians, 2) Destruction of civil objects, 3) Rape, torture, executions. You need to return just either \"Yes\" or \"No\" label. \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Get labels\n",
        "labels_eng_prompt = label_with_chatgpt(prompt, data)\n",
        "\n",
        "def normalize_label(labels):\n",
        "  out = []\n",
        "  for label in labels:\n",
        "    label = label.lower()  # Normalize to lowercase\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label:  # Check for relevant terms\n",
        "      out.append('Yes')\n",
        "    else:\n",
        "      out.append('No')\n",
        "  return out\n",
        "\n",
        "# Apply the normalization function to the list\n",
        "labels_eng_prompt = normalize_label(labels_eng_prompt)\n",
        "\n",
        "labels_eng_prompt = pd.DataFrame(labels_eng_prompt)\n",
        "\n",
        "\n",
        "# evaluation\n",
        "\n",
        "# encoding\n",
        "#labels_eng_prompt['labels'] = labels_eng_prompt[0].replace({'Yes': 1, 'No': 0})\n",
        "gpt_labels = labels_eng_prompt[0].to_list()\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_true = human_labels\n",
        "y_pred = gpt_labels\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "print(\"Model: \", name)\n",
        "print(\"Sample size: \", n)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "recall = recall_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "f1 = f1_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': gpt_labels,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8E-GwO23nEL"
      },
      "source": [
        "#### GPT 3.5 One Shot, rus prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnLjOK9D3mDA",
        "outputId": "f6c8ed5c-37c8-47ee-8d67-4f16bb11e395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  GPT_3_5_One_Shot_rus_prompt\n",
            "Sample size:  1000\n",
            "Accuracy: 0.723\n",
            "Precision: 0.6851851851851852\n",
            "Recall: 0.8588007736943907\n",
            "F1 Score: 0.7622317596566524\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 5min 44s (started: 2024-12-08 18:49:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# GPT 3.5 One Shot Learning -- ENG instructions, NO EXAMPLES\n",
        "\n",
        "# started with $11.90, ended with $11.76, 5min44sec\n",
        "import openai\n",
        "\n",
        "name = 'GPT_3_5_One_Shot_rus_prompt'\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "def label_with_chatgpt(prompt, data):\n",
        "    responses = []\n",
        "    for item in data:\n",
        "        full_prompt = f\"{prompt}\\n\\nData: {item}\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "            max_tokens=50\n",
        "        )\n",
        "        label = response.choices[0].message['content'].strip()\n",
        "        responses.append(label)\n",
        "    return responses\n",
        "\n",
        "\n",
        "### ALL RUSSIAN PROMPT\n",
        "prompt = \"\"\"Инструкция: Вы — современная гуманитарная онлайн система для обнаружения нарушений прав человека.\n",
        "Получив публикацию в социальных сетях, ваша задача — классифицировать, содержит ли она упоминания о нарушениях прав человека.\n",
        "Нарушения прав человека включают (но не ограничиваются): 1) Убийство мирных жителей, 2) Уничтожение гражданских объектов,\n",
        "3) Изнасилование, пытки, казни. Вы должны возвращать только лейбл \"Да\" или \"Нет\" \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Get labels\n",
        "labels_eng_prompt = label_with_chatgpt(prompt, data)\n",
        "\n",
        "def normalize_label(labels):\n",
        "  out = []\n",
        "  for label in labels:\n",
        "    label = label.lower()  # Normalize to lowercase\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label:  # Check for relevant terms\n",
        "      out.append('Yes')\n",
        "    else:\n",
        "      out.append('No')\n",
        "  return out\n",
        "\n",
        "# Apply the normalization function to the list\n",
        "labels_eng_prompt = normalize_label(labels_eng_prompt)\n",
        "labels_eng_prompt = pd.DataFrame(labels_eng_prompt)\n",
        "\n",
        "\n",
        "# evaluation\n",
        "\n",
        "# encoding\n",
        "#labels_eng_prompt['labels'] = labels_eng_prompt[0].replace({'Yes': 1, 'No': 0})\n",
        "gpt_labels = labels_eng_prompt[0].to_list()\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_true = human_labels\n",
        "y_pred = gpt_labels\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "print(\"Model: \", name)\n",
        "print(\"Sample size: \", n)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "recall = recall_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "f1 = f1_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': gpt_labels,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lw0vK-fphop"
      },
      "source": [
        "### GPT 4o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD82RKqg0fQH"
      },
      "source": [
        "####GPT 4o Few Shot, eng prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnmURz6Aohn2",
        "outputId": "ab6f9550-534a-42f6-96a2-83cb08016ce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  GPT_4o_Few_Shot_eng_prompt\n",
            "Sample size:  1000\n",
            "Accuracy: 0.823\n",
            "Precision: 0.8556485355648535\n",
            "Recall: 0.7911025145067698\n",
            "F1 Score: 0.8221105527638191\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 7min 16s (started: 2024-12-08 18:57:39 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#  GPT 4o Few Shot Learning -- ENG instructions, WITH EXAMPLES\n",
        "\n",
        "# started with $11.76, ended with $10.58, took 7min 16s\n",
        "\n",
        "import openai\n",
        "\n",
        "name = 'GPT_4o_Few_Shot_eng_prompt'\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "def label_with_chatgpt(prompt, data):\n",
        "    responses = []\n",
        "    for item in data:\n",
        "        full_prompt = f\"{prompt}\\n\\nData: {item}\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "            max_tokens=50\n",
        "        )\n",
        "        label = response.choices[0].message['content'].strip()\n",
        "        responses.append(label)\n",
        "    return responses\n",
        "\n",
        "\n",
        "### ALL ENGLIGH PROMPT\n",
        "prompt = \"\"\"Instruction: You are a humanitarian state-of-the-art Human Rights Violation (HRV) detection system. Given a social media post, your job is to classify\n",
        "if the post contains HRV mentions. HRV includes (but not limited to): 1) Killing civilians, 2) Destruction of civil objects, 3) Rape, torture, executions. You need to return just either \"Yes\" or \"No\" label.\n",
        "Here are some examples:\n",
        "\n",
        "Example 1\n",
        "\n",
        "Post: In Mykolaiv, after the arrival at the house, there are already five dead…\n",
        "Label: Yes\n",
        "\n",
        "Example 2\n",
        "\n",
        "Post: Russian aviation struck the Neptune pool in Mariupol where there were pregnant women and mothers with children under 3 years old. Orcs fighting against civilians - a special cauldron in hell is prepared for you. Informer | Subscribe\n",
        "Label: Yes\n",
        "\n",
        "Example 3\n",
        "\n",
        "Post: The first episode is played. There will be others. And more. I will express a personal position. I cannot fail to mention this now. The Ukrainian state in its current configuration with a Nazi political regime will pose a constant, direct, and clear threat to Russia. Therefore, in addition to protecting our people and guarding the country's borders, the goal of our future actions, in my opinion, should be the full dismantling of the political regime in Ukraine.\n",
        "Label: No\n",
        "\n",
        "Note that the destruction of military objects (for example, an airbase) is not considered HRV.\n",
        "\n",
        "You need to putput label only.\"\"\"\n",
        "\n",
        "\n",
        "#data = data['Post'].to_list()\n",
        "\n",
        "# Get labels\n",
        "labels_eng_prompt = label_with_chatgpt(prompt, data)\n",
        "\n",
        "def normalize_label(labels):\n",
        "  out = []\n",
        "  for label in labels:\n",
        "    label = label.lower()  # Normalize to lowercase\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label:  # Check for relevant terms\n",
        "      out.append('Yes')\n",
        "    else:\n",
        "      out.append('No')\n",
        "  return out\n",
        "\n",
        "# Apply the normalization function to the list\n",
        "labels_eng_prompt = normalize_label(labels_eng_prompt)\n",
        "\n",
        "labels_eng_prompt = pd.DataFrame(labels_eng_prompt)\n",
        "\n",
        "#labels_eng_prompt.to_csv('/content/gdrive/My Drive/HRV NLP Project X/Data/Data [January 2024]/gpt_labels_eng_prompt_hrv.csv')\n",
        "\n",
        "\n",
        "# evaluation\n",
        "\n",
        "# encoding\n",
        "#labels_eng_prompt['labels'] = labels_eng_prompt[0].replace({'Yes': 1, 'No': 0})\n",
        "gpt_labels = labels_eng_prompt[0].to_list()\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_true = human_labels\n",
        "y_pred = gpt_labels\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "print(\"Model: \", name)\n",
        "print(\"Sample size: \", n)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "recall = recall_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "f1 = f1_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': gpt_labels,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab1PLGiE4vBS"
      },
      "source": [
        "####GPT 4o Few Shot, rus prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as1LkDwu4ttZ",
        "outputId": "44ee3729-9856-468e-9328-87d9b90cd165"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  GPT_4o_Few_Shot_rus_prompt\n",
            "Sample size:  1000\n",
            "Accuracy: 0.828\n",
            "Precision: 0.8709677419354839\n",
            "Recall: 0.7833655705996132\n",
            "F1 Score: 0.824847250509165\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 5min 40s (started: 2024-12-08 19:06:14 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#  GPT 4o Few Shot Learning -- ENG instructions, WITH EXAMPLES\n",
        "\n",
        "# started with $10.58, ended with $9.28, took 5min 40s\n",
        "\n",
        "import openai\n",
        "\n",
        "name = 'GPT_4o_Few_Shot_rus_prompt'\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "def label_with_chatgpt(prompt, data):\n",
        "    responses = []\n",
        "    for item in data:\n",
        "        full_prompt = f\"{prompt}\\n\\nData: {item}\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "            max_tokens=50\n",
        "        )\n",
        "        label = response.choices[0].message['content'].strip()\n",
        "        responses.append(label)\n",
        "    return responses\n",
        "\n",
        "\n",
        "### ALL RUSSIAN PROMPT\n",
        "\n",
        "prompt = \"\"\"Инструкция: Вы — современная гуманитарная онлайн система для обнаружения нарушений прав человека. Получив публикацию в социальных сетях, ваша задача — классифицировать,\n",
        "содержит ли она упоминания о нарушениях прав человека. Нарушения включают (но не ограничиваются): 1) Убийство мирных жителей, 2) Уничтожение гражданских объектов, 3) Изнасилование, пытки, казни.\n",
        "Вы должны возвращать только лейбл \"Да\" или \"Нет\".\n",
        "\n",
        "Примеры:\n",
        "\n",
        "Пример 1\n",
        "\n",
        "Пост: В Николаеве, после прилёта в дом, уже пятеро погибших...\n",
        "лейбл: Да\n",
        "\n",
        "Пример 2\n",
        "\n",
        "Пост: Российская авиация нанесла удар по бассейну «Нептун» в Мариуполе, где находились беременные женщины и матери с детьми до 3 лет. Орки, воюющие против мирных жителей, — для вас в аду приготовлен особый котел. Информатор | Подписывайтесь.\n",
        "лейбл: Да\n",
        "\n",
        "Пример 3\n",
        "\n",
        "Пост: Первый эпизод сыгран. Будут другие. И еще. Я выражу личную позицию. Не могу не упомянуть об этом сейчас. Украинское государство в его нынешней конфигурации с нацистским политическим режимом будет представлять постоянную, прямую и явную угрозу для России. Поэтому, помимо защиты нашего народа и охраны границ страны, целью наших будущих действий, по моему мнению, должно стать полное демонтаж политического режима в Украине.\n",
        "лейбл: Нет\n",
        "\n",
        "Учтите, что уничтожение военных объектов (например, авиабазы) не считается нарушением прав человека.\n",
        "\n",
        "Вы должны предоставить только лейбл.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Get labels\n",
        "labels_eng_prompt = label_with_chatgpt(prompt, data)\n",
        "\n",
        "def normalize_label(labels):\n",
        "  out = []\n",
        "  for label in labels:\n",
        "    label = label.lower()  # Normalize to lowercase\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label:  # Check for relevant terms\n",
        "      out.append('Yes')\n",
        "    else:\n",
        "      out.append('No')\n",
        "  return out\n",
        "\n",
        "# Apply the normalization function to the list\n",
        "labels_eng_prompt = normalize_label(labels_eng_prompt)\n",
        "\n",
        "labels_eng_prompt = pd.DataFrame(labels_eng_prompt)\n",
        "\n",
        "#labels_eng_prompt.to_csv('/content/gdrive/My Drive/HRV NLP Project X/Data/Data [January 2024]/gpt_labels_eng_prompt_hrv.csv')\n",
        "\n",
        "\n",
        "# evaluation\n",
        "\n",
        "# encoding\n",
        "#labels_eng_prompt['labels'] = labels_eng_prompt[0].replace({'Yes': 1, 'No': 0})\n",
        "gpt_labels = labels_eng_prompt[0].to_list()\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_true = human_labels\n",
        "y_pred = gpt_labels\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "print(\"Model: \", name)\n",
        "print(\"Sample size: \", n)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "recall = recall_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "f1 = f1_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': gpt_labels,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIOl32xa0-6k"
      },
      "source": [
        "####GPT 4o One Shot, eng prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QQ-aGqYu4V8",
        "outputId": "1cbd85d5-903e-48f4-c3b7-58c0b2328294"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  GPT_4o_One_Shot_eng_prompt\n",
            "Sample size:  1000\n",
            "Accuracy: 0.823\n",
            "Precision: 0.7777777777777778\n",
            "Recall: 0.9206963249516441\n",
            "F1 Score: 0.8432240921169176\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 7min 7s (started: 2024-12-08 19:12:59 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#  GPT 4o One Shot Learning -- ENG instructions, NO EXAMPLES\n",
        "\n",
        "# started with $9.28, ended with $8.72\n",
        "import openai\n",
        "\n",
        "name = 'GPT_4o_One_Shot_eng_prompt'\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "def label_with_chatgpt(prompt, data):\n",
        "    responses = []\n",
        "    for item in data:\n",
        "        full_prompt = f\"{prompt}\\n\\nData: {item}\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "            max_tokens=50\n",
        "        )\n",
        "        label = response.choices[0].message['content'].strip()\n",
        "        responses.append(label)\n",
        "    return responses\n",
        "\n",
        "\n",
        "### ALL ENGLIGH PROMPT\n",
        "prompt = \"\"\"Instruction: You are a humanitarian state-of-the-art Human Rights Violation (HRV) detection system. Given a social media post, your job is to classify\n",
        "if the post contains HRV mentions. HRV includes (but not limited to): 1) Killing civilians, 2) Destruction of civil objects, 3) Rape, torture, executions. You need to return just either \"Yes\" or \"No\" label. \"\"\"\n",
        "\n",
        "\n",
        "# Get labels\n",
        "labels_eng_prompt = label_with_chatgpt(prompt, data)\n",
        "\n",
        "def normalize_label(labels):\n",
        "  out = []\n",
        "  for label in labels:\n",
        "    label = label.lower()  # Normalize to lowercase\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label:  # Check for relevant terms\n",
        "      out.append('Yes')\n",
        "    else:\n",
        "      out.append('No')\n",
        "  return out\n",
        "\n",
        "# Apply the normalization function to the list\n",
        "labels_eng_prompt = normalize_label(labels_eng_prompt)\n",
        "\n",
        "labels_eng_prompt = pd.DataFrame(labels_eng_prompt)\n",
        "\n",
        "#labels_eng_prompt.to_csv('/content/gdrive/My Drive/HRV NLP Project X/Data/Data [January 2024]/gpt_labels_eng_prompt_hrv.csv')\n",
        "\n",
        "\n",
        "# evaluation\n",
        "\n",
        "# encoding\n",
        "#labels_eng_prompt['labels'] = labels_eng_prompt[0].replace({'Yes': 1, 'No': 0})\n",
        "gpt_labels = labels_eng_prompt[0].to_list()\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_true = human_labels\n",
        "y_pred = gpt_labels\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "print(\"Model: \", name)\n",
        "print(\"Sample size: \", n)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "recall = recall_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "f1 = f1_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': gpt_labels,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjaiaGJr6weV"
      },
      "source": [
        "####GPT 4o One Shot, rus prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em7hHwGl60hH",
        "outputId": "95a9891d-ddef-42d4-b9b9-36409b557c86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  GPT_4o_One_Shot_rus_prompt\n",
            "Sample size:  1000\n",
            "Accuracy: 0.807\n",
            "Precision: 0.7709030100334449\n",
            "Recall: 0.8916827852998066\n",
            "F1 Score: 0.8269058295964126\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 8min 27s (started: 2024-12-08 19:22:13 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#  GPT 4o One Shot Learning -- ENG instructions, NO EXAMPLES\n",
        "\n",
        "# started with $8.72, ended with $8.10, took 8min 27s\n",
        "import openai\n",
        "\n",
        "name = 'GPT_4o_One_Shot_rus_prompt'\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "def label_with_chatgpt(prompt, data):\n",
        "    responses = []\n",
        "    for item in data:\n",
        "        full_prompt = f\"{prompt}\\n\\nData: {item}\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "            max_tokens=50\n",
        "        )\n",
        "        label = response.choices[0].message['content'].strip()\n",
        "        responses.append(label)\n",
        "    return responses\n",
        "\n",
        "\n",
        "### ALL RUSSIAN PROMPT\n",
        "prompt = \"\"\"Инструкция: Вы — современная гуманитарная онлайн система для обнаружения нарушений прав человека.\n",
        "Получив публикацию в социальных сетях, ваша задача — классифицировать, содержит ли она упоминания о нарушениях прав человека.\n",
        "Нарушения прав человека включают (но не ограничиваются): 1) Убийство мирных жителей, 2) Уничтожение гражданских объектов,\n",
        "3) Изнасилование, пытки, казни. Вы должны возвращать только лейбл \"Да\" или \"Нет\" \"\"\"\n",
        "\n",
        "\n",
        "# Get labels\n",
        "labels_eng_prompt = label_with_chatgpt(prompt, data)\n",
        "\n",
        "def normalize_label(labels):\n",
        "  out = []\n",
        "  for label in labels:\n",
        "    label = label.lower()  # Normalize to lowercase\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label:  # Check for relevant terms\n",
        "      out.append('Yes')\n",
        "    else:\n",
        "      out.append('No')\n",
        "  return out\n",
        "\n",
        "# Apply the normalization function to the list\n",
        "labels_eng_prompt = normalize_label(labels_eng_prompt)\n",
        "\n",
        "labels_eng_prompt = pd.DataFrame(labels_eng_prompt)\n",
        "\n",
        "# evaluation\n",
        "\n",
        "# encoding\n",
        "#labels_eng_prompt['labels'] = labels_eng_prompt[0].replace({'Yes': 1, 'No': 0})\n",
        "gpt_labels = labels_eng_prompt[0].to_list()\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_true = human_labels\n",
        "y_pred = gpt_labels\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "print(\"Model: \", name)\n",
        "print(\"Sample size: \", n)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "recall = recall_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "f1 = f1_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': gpt_labels,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te7WSwfsMVcw"
      },
      "source": [
        "### LLAMA 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weDr4WQceHis"
      },
      "source": [
        "####Accessing HuggingFace Gated Repos (Needed for LLAMA and Mistral)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIa6urATap7Z",
        "outputId": "17dd423f-9dee-4334-88d3-1ce81294859b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 104 ms (started: 2024-12-08 19:32:40 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!git config --global credential.helper store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXT1Y17aZeYu",
        "outputId": "f8167c4d-0ed1-487a-a8d5-909694b0db8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `LLM annotations study` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `LLM annotations study`\n",
            "time: 19 s (started: 2024-12-08 19:32:42 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEe_CQdUehaF"
      },
      "source": [
        "#### Setting up 8-bit quantization (needed for LLAMA and Mistral)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTr0x4RobAYV",
        "outputId": "3ee91572-2a2b-4c6f-f151-cbe8de5e2c4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.0\n",
            "time: 5.03 s (started: 2024-12-08 19:33:17 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeGV068gb3jx"
      },
      "outputs": [],
      "source": [
        "#!pip uninstall bitsandbytes transformers\n",
        "#!pip install transformers bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNgaAwDl1FIs"
      },
      "source": [
        "####Llama 3 One shot, eng prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdouOwbK8gmW",
        "outputId": "040643a9-7e5e-4b20-f08e-6caa0f2b63e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  LLama3_One_Shot_eng_prompt\n",
            "Accuracy: 0.484\n",
            "Precision: 1.0\n",
            "Recall: 0.0019342359767891683\n",
            "F1 Score: 0.003861003861003861\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 47.8 ms (started: 2024-12-08 19:38:48 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Llama 3 One shot learning -- ENG prompt, NO EXAMPLES\n",
        "\n",
        "name = 'LLama3_One_Shot_eng_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with the new quantization config\n",
        "\n",
        "print(\"Loading model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "''' # if I want to use the model withought 8-bit quantization\n",
        "print(\"Loading model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using LLaMA\n",
        "def one_shot_annotate(text):\n",
        "\n",
        "  prompt = f\"\"\"Instruction: You are a humanitarian state-of-the-art Human Rights Violation (HRV) detection system.\n",
        "  Given a social media post, your job is to classify if the post contains HRV mentions. HRV includes (but not limited to):\n",
        "  1) Killing civilians, 2) Destruction of civil objects, 3) Rape, torture, executions.\n",
        "  Please classify this post: {text}. You need to return just either \"Yes\" or \"No\" label. \"\"\"\n",
        "\n",
        "\n",
        "  # Generate label using LLaMA\n",
        "  response = llama_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using LLaMA\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "llama_labels = annotations\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, llama_labels)\n",
        "precision = precision_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRDEmlR0yGza",
        "outputId": "0d8e8ef6-a81c-4727-e000-364253f5910a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model with 8-bit quantization...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  LLama3_One_Shot_eng_prompt\n",
            "Accuracy: 0.486\n",
            "Precision: 0.5157894736842106\n",
            "Recall: 0.09477756286266925\n",
            "F1 Score: 0.16013071895424835\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 3min 32s (started: 2024-12-08 19:40:34 +00:00)\n"
          ]
        }
      ],
      "source": [
        "### CHANGING PROMPT v2 ###\n",
        "\n",
        "# Llama 3 One shot learning -- ENG prompt, NO EXAMPLES\n",
        "\n",
        "name = 'LLama3_One_Shot_eng_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with the new quantization config\n",
        "\n",
        "print(\"Loading model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "''' # if I want to use the model withought 8-bit quantization\n",
        "print(\"Loading model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9  # Ensure deterministic output\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using LLaMA\n",
        "def one_shot_annotate(text):\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Reading this social media post: \"{text}\", is is clear that it describes human rights violation case. Yes or no?\n",
        "  Label (only \"Yes\" or \"No\") \"\"\"\n",
        "\n",
        "\n",
        "  # Generate label using LLaMA\n",
        "  response = llama_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using LLaMA\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "llama_labels = annotations\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, llama_labels)\n",
        "precision = precision_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_0prMqzzvH_",
        "outputId": "e34ab995-938c-4a23-e03f-db2f27abe9aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model with 8-bit quantization...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  LLama3_One_Shot_eng_prompt\n",
            "Accuracy: 0.48\n",
            "Precision: 0.42857142857142855\n",
            "Recall: 0.017408123791102514\n",
            "F1 Score: 0.03345724907063197\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 3min 35s (started: 2024-12-08 19:49:21 +00:00)\n"
          ]
        }
      ],
      "source": [
        "### ANOTHER DIFFERENT PROMPT v3 ###\n",
        "\n",
        "\n",
        "# Llama 3 One shot learning -- ENG prompt, NO EXAMPLES\n",
        "\n",
        "name = 'LLama3_One_Shot_eng_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with the new quantization config\n",
        "\n",
        "print(\"Loading model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "''' # if I want to use the model withought 8-bit quantization\n",
        "print(\"Loading model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9  # Ensure deterministic output\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using LLaMA\n",
        "def one_shot_annotate(text):\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  You are an expert in identifying human rights violations in social media posts. Your task is\n",
        "  to analyze the provided post and determine whether it refers to a human rights violation.\n",
        "\n",
        "  Read the following post carefully:\n",
        "  \"{text}\"\n",
        "\n",
        "  Respond with a single word: \"Yes\" if the post describes a human rights violation, or \"No\" if it\n",
        "  does not. Avoid providing explanations or additional comments.\n",
        "\n",
        "  Label (only \"Yes\" or \"No\"): \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "  # Generate label using LLaMA\n",
        "  response = llama_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using LLaMA\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "llama_labels = annotations\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, llama_labels)\n",
        "precision = precision_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABk-NJSa15BX",
        "outputId": "32695f34-39a4-46e3-ed58-bb1dcbbe8e3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model with 8-bit quantization...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  LLama3_One_Shot_eng_prompt\n",
            "Accuracy: 0.516\n",
            "Precision: 0.5169230769230769\n",
            "Recall: 0.9748549323017408\n",
            "F1 Score: 0.675603217158177\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 3min 37s (started: 2024-12-08 19:56:39 +00:00)\n"
          ]
        }
      ],
      "source": [
        "### BEST PERFORMING PROMPT ###\n",
        "\n",
        "### ANOTHER DIFFERENT PROMPT - v4 ###\n",
        "\n",
        "\n",
        "# Llama 3 One shot learning -- ENG prompt, NO EXAMPLES\n",
        "\n",
        "name = 'LLama3_One_Shot_eng_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with the new quantization config\n",
        "\n",
        "print(\"Loading model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "''' # if I want to use the model withought 8-bit quantization\n",
        "print(\"Loading model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using LLaMA\n",
        "def one_shot_annotate(text):\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Does this social media post describe a human rights violation?\n",
        "  Post: \"{text}\".\n",
        "  Respond with \"Yes\" or \"No\" only.\n",
        "  Label: \"\"\"\n",
        "\n",
        "\n",
        "  # Generate label using LLaMA\n",
        "  response = llama_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using LLaMA\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "llama_labels = annotations\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, llama_labels)\n",
        "precision = precision_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC89VIaS93lu"
      },
      "source": [
        "####Llama 3 One shot, rus prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BSqdiiC92fn",
        "outputId": "02061a90-13a9-4686-d48f-1e9cfd372e50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model with 8-bit quantization...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  Llama3_One_shot_rus_prompt\n",
            "Accuracy: 0.499\n",
            "Precision: 0.55\n",
            "Recall: 0.1702127659574468\n",
            "F1 Score: 0.25997045790251105\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 1min 55s (started: 2024-12-08 20:39:34 +00:00)\n"
          ]
        }
      ],
      "source": [
        "### BEST PERFORMING PROMPT ###\n",
        "\n",
        "# Llama 3 One shot learning -- ENG prompt, NO EXAMPLES\n",
        "\n",
        "name = 'Llama3_One_shot_rus_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with the new quantization config\n",
        "\n",
        "print(\"Loading model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "''' # if I want to use the model withought 8-bit quantization\n",
        "print(\"Loading model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using LLaMA\n",
        "def one_shot_annotate(text):\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Ваша задача — определить, содержит ли пост из соцсети упоминания о нарушениях прав человека.\n",
        "\n",
        "  Пост: \"{text}\"\n",
        "  Предоставьте лейбл (только \"Yes\" или \"No\")\n",
        "  \"\"\"\n",
        "\n",
        "  # Generate label using LLaMA\n",
        "  response = llama_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using LLaMA\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "llama_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, llama_labels)\n",
        "precision = precision_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bhcNbL268gT",
        "outputId": "5174acaf-d575-4d70-989d-1c191ff76204"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model with 8-bit quantization...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  Llama3_One_shot_rus_prompt\n",
            "Accuracy: 0.483\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1 Score: 0.0\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 1min 39s (started: 2024-12-08 20:47:20 +00:00)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "### DIFFERENT RPOMT, v2 ###\n",
        "\n",
        "# Llama 3 One shot learning -- ENG prompt, NO EXAMPLES\n",
        "\n",
        "name = 'Llama3_One_shot_rus_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with the new quantization config\n",
        "\n",
        "print(\"Loading model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "''' # if I want to use the model withought 8-bit quantization\n",
        "print(\"Loading model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using LLaMA\n",
        "def one_shot_annotate(text):\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Вы — система для обнаружения нарушений прав человека. Нарушения включают:\n",
        "  1) Убийство мирных жителей, 2) Уничтожение гражданских объектов, 3) Изнасилование, пытки, казни.\n",
        "  Классифицируйте пост на наличие нарушений прав человека: \"{text}\". Верните только \"Yes\" или \"No\".\n",
        "  \"\"\"\n",
        "\n",
        "  # Generate label using LLaMA\n",
        "  response = llama_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using LLaMA\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "llama_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, llama_labels)\n",
        "precision = precision_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bT-nOLL1Ilq"
      },
      "source": [
        "####Llama 3 Few shot, eng prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qn2FhWDwDsV",
        "outputId": "80e7b554-8406-45f3-e956-1c9ff09a8fc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  Llama3_Few_shot_eng_prompt\n",
            "Accuracy: 0.526\n",
            "Precision: 0.52218782249742\n",
            "Recall: 0.9787234042553191\n",
            "F1 Score: 0.6810228802153432\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 44 ms (started: 2024-12-08 21:15:34 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Llama 3 Few shot learning -- ENG prompt, WITH EXAMPLES\n",
        "\n",
        "name = 'Llama3_Few_shot_eng_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "#from datasets import load_dataset\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with the new quantization config\n",
        "\n",
        "print(\"Loading model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "''' # if I want to use the model withought 8-bit quantization\n",
        "print(\"Loading model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using LLaMA\n",
        "def one_shot_annotate(text):\n",
        "  prompt = f\"\"\"Instruction: You are a humanitarian state-of-the-art Human Rights Violation (HRV) detection system. Given a social media post, your job is to classify\n",
        "  if the post contains HRV mentions. HRV includes (but not limited to): 1) Killing civilians, 2) Destruction of civil objects, 3) Rape, torture, executions. You need to return just either \"Yes\" or \"No\" label.\n",
        "  Here are some examples:\n",
        "\n",
        "  Example 1\n",
        "  Post: In Mykolaiv, after the arrival at the house, there are already five dead…\n",
        "  Label: Yes\n",
        "\n",
        "  Example 2\n",
        "  Post: Russian aviation struck the Neptune pool in Mariupol where there were pregnant women and mothers with children under 3 years old. Orcs fighting against civilians - a special cauldron in hell is prepared for you. Informer | Subscribe\n",
        "  Label: Yes\n",
        "\n",
        "  Example 3\n",
        "  Post: The first episode is played. There will be others. And more. I will express a personal position. I cannot fail to mention this now. The Ukrainian state in its current configuration with a Nazi political regime will pose a constant, direct, and clear threat to Russia. Therefore, in addition to protecting our people and guarding the country's borders, the goal of our future actions, in my opinion, should be the full dismantling of the political regime in Ukraine.\n",
        "  Label: No\n",
        "\n",
        "  Note that the destruction of military objects (for example, an airbase) is not considered HRV.\n",
        "  Post: {text}\n",
        "  Label (only \"Yes\" or \"No\"):\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Generate label using LLaMA\n",
        "  response = llama_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using LLaMA\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "llama_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, llama_labels)\n",
        "precision = precision_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "\n",
        "\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AujLlAbfEITt"
      },
      "source": [
        "#### Llama 3 Few shot, rus prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVk7toaQEHUa",
        "outputId": "e9bec768-6cc2-4813-e8b9-724dda1eaefc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model with 8-bit quantization...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  Llama3_Few_shot_rus_prompt\n",
            "Accuracy: 0.515\n",
            "Precision: 0.5160642570281124\n",
            "Recall: 0.9941972920696325\n",
            "F1 Score: 0.6794448116325181\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 3min 42s (started: 2024-12-08 21:27:52 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Llama 3 Few shot learning -- RUS prompt, WITH EXAMPLES\n",
        "\n",
        "name = 'Llama3_Few_shot_rus_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "#from datasets import load_dataset\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with the new quantization config\n",
        "\n",
        "print(\"Loading model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "''' # if I want to use the model withought 8-bit quantization\n",
        "print(\"Loading model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using LLaMA\n",
        "def one_shot_annotate(text):\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Инструкция: Вы — гуманитарная система для обнаружения нарушений прав человека. Ваша задача — классифицировать, содержит ли публикация в социальных сетях упоминания о нарушениях прав человека.\n",
        "\n",
        "  Что считается нарушением прав человека:\n",
        "  - Убийство мирных жителей\n",
        "  - Уничтожение гражданских объектов\n",
        "  - Изнасилование, пытки, казни\n",
        "\n",
        "  Пример 1:\n",
        "  Пост: В Николаеве, после прилёта в дом, уже пятеро погибших...\n",
        "  Лейбл: Да\n",
        "\n",
        "  Пример 2:\n",
        "  Пост: Российская авиация нанесла удар по бассейну «Нептун» в Мариуполе, где находились беременные женщины и матери с детьми до 3 лет. Орки, воюющие против мирных жителей, — для вас в аду приготовлен особый котел. Информатор | Подписывайтесь.\n",
        "  Лейбл: Да\n",
        "\n",
        "  Пример 3:\n",
        "  Пост: Первый эпизод сыгран. Будут другие. И еще. Я выражу личную позицию. Не могу не упомянуть об этом сейчас. Украинское государство в его нынешней конфигурации с нацистским политическим режимом будет представлять постоянную, прямую и явную угрозу для России. Поэтому, помимо защиты нашего народа и охраны границ страны, целью наших будущих действий, по моему мнению, должно стать полное демонтаж политического режима в Украине.\n",
        "  Лейбл: Нет\n",
        "\n",
        "  Теперь классифицируйте следующий пост:\n",
        "  Пост: \"{text}\"\n",
        "\n",
        "  Ответьте только \"Да\" или \"Нет\":\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "  # Generate label using LLaMA\n",
        "  response = llama_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label or 'Yes' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using LLaMA\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "llama_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, llama_labels)\n",
        "precision = precision_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io6rwEcFH3RS"
      },
      "source": [
        "### MISTRAL-7B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuFIIG2j1McJ"
      },
      "source": [
        "####Mistral-7B One shot, eng prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fb4e8b3cc1bb4d4c8835c3f39d33db54",
            "4513d2be315d4463be76ab919ce46b78",
            "92856b9799144623bd58b3e1d52fa3ae",
            "3a94acf08e0646e39b53aef79abf6428",
            "10790ac730654ce28696d55f1eddbd7b",
            "6b2efd41b3fd4121b7a83e5380738ea0",
            "84ab83fa39184b9cb62f8d587055fdcd",
            "d2b579d31b914204a30f757eeb4d42c1",
            "5930e72a6f194a5aa1d1a7848e0d3792",
            "f8ef6ac453ef451380b80c04e4b9f8a8",
            "04f6f4dc98854784a26cc8e2b14f4487",
            "f0903d60aa824f87bf1c97e749302cc7",
            "a14ec6a9cbc340b4879b7d84038db346",
            "2d2a1427da004f74a2e6634711f73f96",
            "b4362d6b909f4ab9b1c9929cd325c71d",
            "e1a3df38a5024d118463eaeace7bdd2e",
            "8f66bc6e840f45e5925b973fc51f644f",
            "c44cc0dde60b4486a9473dbe82a7dd97",
            "d41eac15929749d9918abf7b60a49a68",
            "4d7e0f4f4abe43a2a2a017c97efaa3a5",
            "e9946d33406d413eab1e7eaa24847888",
            "7e3eb7b9b97841b8ba7c4c18ef8a8b95",
            "2a4f55ebb08540169ec9fd66722d54aa",
            "589fdd02af3c4be99b49c5934b427398",
            "fec148e84d7e410fb9434b142c81a542",
            "ccc2b8d1b3bb454282ac70b6f6ecea3d",
            "fe059c2f8e4b4ab8a6593e764da78a62",
            "6a9b6faff37d4c8582eecc92e38d5c0c",
            "1ae6dd682c444253a30642d24c4cf715",
            "cc8648e9445c4a33953b589eb6f489c4",
            "75cf74fc723246c79ad58959107f6f89",
            "9ad982b224f34de1831455021caee99b",
            "ab1d696b35d64a7ca24ef37c3bcded51",
            "49f888ec343643189a98c15d8697a3f9",
            "6d5192b4eb3c431aab4affc10ab973cc",
            "acfed305e83747d6ae42de6c9d978f4b",
            "a6cb7b8478cd4a9ba4c9d0cb7f808bf9",
            "830a82825b4449049fd2db141103de97",
            "66b4040f2c0843158efdb13f5b34836b",
            "d256260559cd41739c6dfa4dd9f56c2f",
            "f16743eef90d4e2a969dfa5517fa2c29",
            "880d343bd4a2480798c1cb437f0eeea4",
            "66316e15798f4bc896a6a94b00ccd8e5",
            "75000ccd3a344cf4b1dfd930f2598a83",
            "39b7c8293d2c492cb4a91a31381ef5e5",
            "421bfc60c6d24382abe8d85a58c94f0b",
            "0736149314fe4ad49719e9ec0228833c",
            "30492072c4424fd19f8b2038a3126770",
            "b750ba7ffc47446b9f2fbcb6fa9a34bf",
            "0596b96437fc49a19fdf816dc49bbe6e",
            "ecfe9dbec0e6447781220bb32d47b107",
            "b5536298221a491e894c81099eede655",
            "94e4335e701a4d31a2068b767380e10b",
            "4655b54677b8496db656fd45892fff35",
            "1421695acb6f4356bb5d10bf9ab3d773",
            "c33c3c34d5c84ae7906c0f1a57bb60b9",
            "50895209ff9f402cbd4cd73c54a0c6a1",
            "0304e380701442748c115abe29b97842",
            "d7af53d2de3f4007b58b99a347925c6a",
            "ea26c88ba4b541b0988b2c9f526e7da4",
            "77287599114d45bba367ca594196d624",
            "0d98fd7fa53f49c89d937db6c817e5ca",
            "9577c4968db34825bd737ab42dc11d89",
            "2d21dfc34c8e4cdd954ece284bd5e395",
            "dd18e7b7a2074b039d4e8a361693628f",
            "8c5dfdce347741aa811d7cdd683d53cb",
            "4690426fca3d4de89684ae0d8325b2de",
            "2f0fa77149764fcb8aa0c5d0b0299fb8",
            "1448e0c612bb4ec5b8cf2f7f7c047073",
            "39ddd4e394ce40cf833aee7848bf348d",
            "9e10195d3d384eb5a524d1bf91443adb",
            "471206904bd644b5a6c2efd6092d31d2",
            "820a33cf15244ce48d411ddf54934b98",
            "5433afe643724c7ba6042579694513c5",
            "d4f4fc328c4843fe84d16fbd104f8b0e",
            "a54afe385b8c4c73a1a26496d219f14d",
            "404861031aa8455b985842f8d6943a90",
            "e0e22ff553aa4124bb1ddc9275a5dbd7",
            "fa34a2c4c8f547d89dc530a9f75e3ae2",
            "efddeff79788444dabf111a036a359f4",
            "c813fae22f9548cdbeed247463e01e1b",
            "516b28a3539b4983ad683da272de1e3f",
            "9015717d6d1b4a08ba526748ead729e1",
            "2f14782213734dbfab090e3bd0fe6474",
            "ac127432b0124ccf9975dcb96d25a955",
            "ba5562c66ab548f1afcdd35c195822b9",
            "c6c49ff0a56043b18b30fc99038f948c",
            "7b51fa5e90aa47658614a89698674551",
            "dd5659f88b68437a878a866eb7332e4e",
            "508f59c5748646888fb90db7bdfe3b0a",
            "3d48f6c073e94b438d3bac174f7fc1de",
            "8321cfbaa21c43949148a1b68d05549f",
            "d8d731b22c81460bb1ed9f3d77e7bd23",
            "615adb06c7644863b725561c0a388062",
            "5cf33c9480094ccea0c9516707309436",
            "7fcfc2e45d8845fa93d8252958264a48",
            "debe8411da6a4587a7b4b2f7678e5f57",
            "7dbce6f459914ecc9354cd5b49ade08e",
            "d0c7005ad0874dbfac6465502baacce2",
            "ff304bd0ae144e6da3ffe872a5f3b9d0",
            "9db9262dcec64dda892d8393e8ca403c",
            "916935281df84b8793bda53e37556147",
            "02afe9fb448e4e0fbd2a1b49d51bc5a7",
            "3662ca4ba4834d98b8a0747e49ff420e",
            "a3e1803fd9cc4086af0670ba3abe1ff4",
            "c5b9f1c971db45e8aecd85aa3d133e7a",
            "0172deab5f504cadb4ee9e74ee0dcc78",
            "608672f0b1fc4c5cb17e9ffb3328a798",
            "ca17c43b081e422ab112e20b25abdd38",
            "db50ad0abc354a1bb5cb25dfbab74d35",
            "d52e04954e1b4053ab1663f687ca1838",
            "95d409bcc5f947afa37ef49d28be6a5f",
            "d19a9b518bff4e8e900588259c18ab8d",
            "9b423df2e924403eb6b4938f1864aa54",
            "9580e5325d9c426b905413bc3afc44cc",
            "5f45333adc004573bba298f0266860d7",
            "04184e88470645f88f28f94345b5fe75",
            "b45f06fa224d4524af1095ab70072951",
            "351d1e1af31944aa91b107fa9f9ad396",
            "172e4ce5775947c8ba81e0fadf2c93d3",
            "19cf4028950c4fa7b45c005212d23c23"
          ]
        },
        "id": "X9yVzVbpH5C-",
        "outputId": "5c4a536b-37a5-4806-ca36-286a771e63d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Mistral model with 8-bit quantization...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb4e8b3cc1bb4d4c8835c3f39d33db54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0903d60aa824f87bf1c97e749302cc7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a4f55ebb08540169ec9fd66722d54aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49f888ec343643189a98c15d8697a3f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39b7c8293d2c492cb4a91a31381ef5e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c33c3c34d5c84ae7906c0f1a57bb60b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4690426fca3d4de89684ae0d8325b2de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0e22ff553aa4124bb1ddc9275a5dbd7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd5659f88b68437a878a866eb7332e4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff304bd0ae144e6da3ffe872a5f3b9d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d52e04954e1b4053ab1663f687ca1838",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  Mistral7B_One_shot_eng_prompt\n",
            "Accuracy: 0.484\n",
            "Precision: 0.6666666666666666\n",
            "Recall: 0.0038684719535783366\n",
            "F1 Score: 0.007692307692307693\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 11min 26s (started: 2024-12-08 21:35:51 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# MISTRAL One shot learning -- ENG prompt, NO EXAMPLES\n",
        "\n",
        "name = 'Mistral7B_One_shot_eng_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "dataset = data  # Replace with your actual dataset\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"  # Update to the Mistral model\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with 8-bit quantization\n",
        "\n",
        "print(\"Loading Mistral model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "# If you want to use the model without 8-bit quantization, uncomment the code below:\n",
        "'''\n",
        "print(\"Loading Mistral model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "mistral_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        "\n",
        "# Define a function for one-shot annotation using Mistral\n",
        "def one_shot_annotate(text):\n",
        "  prompt = f\"\"\"Instruction: You are a humanitarian state-of-the-art Human Rights Violation (HRV) detection system.\n",
        "  Given a social media post, your job is to classify if the post contains HRV mentions. HRV includes (but not limited to):\n",
        "  1) Killing civilians, 2) Destruction of civil objects, 3) Rape, torture, executions.\n",
        "  You need to return just either \"Yes\" or \"No\" label: {text}\"\"\"\n",
        "\n",
        "  # Generate label using Mistral\n",
        "  response = mistral_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label or 'Yes' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using Mistral\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "mistral_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, mistral_labels)\n",
        "precision = precision_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "54db2a7091f24dc98cc697e73d1c0335",
            "a23d8276924644629a85fe2d9c74a5d6",
            "84e60abe83c7487baee99593f41518cc",
            "27fafdad7aa449c59e0f0785a17606cd",
            "5877a5dbf79645fe849b28b9e6b2e604",
            "b84edcef9ed547e7b69f5c36bc8aeff2",
            "b4a8caea1d9941f5ae09900b2913c7ea",
            "ff50c5315f3c43a094cc8c4cfec34950",
            "19680ba1c75c4eceab061877c18426f2",
            "286e58e910c8412f867150703a0d78ba",
            "df2aafa28aab46d6bfe889e06fb90a20"
          ]
        },
        "id": "o8juX62BVhY0",
        "outputId": "f53556af-a387-4bf9-9b8e-0410a85b55ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Mistral model with 8-bit quantization...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54db2a7091f24dc98cc697e73d1c0335",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  Mistral7B_One_shot_rus_prompt\n",
            "Accuracy: 0.496\n",
            "Precision: 0.6756756756756757\n",
            "Recall: 0.048355899419729204\n",
            "F1 Score: 0.09025270758122744\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 10min 8s (started: 2024-12-08 22:15:33 +00:00)\n"
          ]
        }
      ],
      "source": [
        "### DIFFERENT PROMPT v2\n",
        "\n",
        "# MISTRAL One shot learning -- ENG prompt, NO EXAMPLES\n",
        "\n",
        "name = 'Mistral7B_One_shot_rus_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "dataset = data  # Replace with your actual dataset\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"  # Update to the Mistral model\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with 8-bit quantization\n",
        "\n",
        "print(\"Loading Mistral model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "# If you want to use the model without 8-bit quantization, uncomment the code below:\n",
        "'''\n",
        "print(\"Loading Mistral model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "mistral_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using Mistral\n",
        "def one_shot_annotate(text):\n",
        "  prompt = f\"\"\"\n",
        "  Ваша задача — определить, содержит ли пост из соцсети упоминания о нарушениях прав человека.\n",
        "\n",
        "  Пост: \"{text}\"\n",
        "  Предоставьте лейбл (только \"Yes\" или \"No\")\n",
        "  \"\"\"\n",
        "\n",
        "  # Generate label using Mistral\n",
        "  response = mistral_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label or 'Yes' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using Mistral\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "mistral_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, mistral_labels)\n",
        "precision = precision_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "f008e08bafe048828cf0ec6122dc665a",
            "6888de350abf43679bcd676939637832",
            "102b8928e0c04152ba7788a56595b34f",
            "0142b9ffd07d4bada8014b1dace686fa",
            "4325c36cb18848748954e7ed14bbd523",
            "3467d096bbf44bd2bf9332a7282e61df",
            "87d1ef463cc74256bbf2211d085f6a29",
            "6002f55035244f8792a2e8f20fbfe8e7",
            "6187b06747f040a0826837e13c696480",
            "752368fdf48c44089727e1c527567001",
            "8a190844e13e498db5b12ebeba0166f6"
          ]
        },
        "id": "S2559O7oZSIC",
        "outputId": "84997a30-4b66-48a1-ac7a-460220a7874e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Mistral model with 8-bit quantization...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f008e08bafe048828cf0ec6122dc665a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  Mistral7B_One_shot_eng_prompt\n",
            "Accuracy: 0.477\n",
            "Precision: 0.4642857142857143\n",
            "Recall: 0.07543520309477757\n",
            "F1 Score: 0.129783693843594\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 10min 6s (started: 2024-12-08 22:31:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "### DIFFERENT PROMPT v3 ###\n",
        "\n",
        "# MISTRAL One shot learning -- ENG prompt, NO EXAMPLES\n",
        "\n",
        "name = 'Mistral7B_One_shot_eng_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "dataset = data  # Replace with your actual dataset\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"  # Update to the Mistral model\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with 8-bit quantization\n",
        "\n",
        "print(\"Loading Mistral model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "# If you want to use the model without 8-bit quantization, uncomment the code below:\n",
        "'''\n",
        "print(\"Loading Mistral model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "mistral_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using Mistral\n",
        "def one_shot_annotate(text):\n",
        "  prompt = f\"\"\"\n",
        "  Reading this social media post: \"{text}\", is is clear that it describes human rights violation case. Yes or No?\n",
        "  Label (only \"Yes\" or \"No\")\n",
        "  \"\"\"\n",
        "\n",
        "  # Generate label using Mistral\n",
        "  response = mistral_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label or 'Yes' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using Mistral\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "mistral_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, mistral_labels)\n",
        "precision = precision_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKnh4G36JySG"
      },
      "source": [
        "#### Mistral-7B One shot, rus prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "c7530c4f59e64d379988a467e87b5f63",
            "14a5e814225b40db8da4c77b75172c5e",
            "a31b01033c394317b0762a519e006a09",
            "adce3e35f65f4ad7a19d3aca9bea617d",
            "01887a2569c84a7391775724bd8f1e9b",
            "1313949c9861425f838d53839a57006a",
            "5d41d35b827141a8b0c4a5b45b78d2a1",
            "a2f22f03f9ff411fa3a86f636aaf3689",
            "6883044661504a40a083eb48dc9f9bda",
            "739e93c24ade4569929d83d6bcad53f8",
            "8ab66c1fc67c43419a79e6b510f0c211"
          ]
        },
        "id": "u42iwDHWJw0G",
        "outputId": "fc89eedd-ccb8-49f1-bf78-08cf73046689"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Mistral model with 8-bit quantization...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7530c4f59e64d379988a467e87b5f63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  Mistral7B_One_shot_rus_prompt\n",
            "Accuracy: 0.484\n",
            "Precision: 0.5238095238095238\n",
            "Recall: 0.02127659574468085\n",
            "F1 Score: 0.040892193308550186\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 10min 15s (started: 2024-12-08 22:45:05 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# MISTRAL One shot learning -- ENG prompt, NO EXAMPLES\n",
        "\n",
        "name = 'Mistral7B_One_shot_rus_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "dataset = data  # Replace with your actual dataset\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"  # Update to the Mistral model\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with 8-bit quantization\n",
        "\n",
        "print(\"Loading Mistral model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "# If you want to use the model without 8-bit quantization, uncomment the code below:\n",
        "'''\n",
        "print(\"Loading Mistral model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "mistral_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using Mistral\n",
        "def one_shot_annotate(text):\n",
        "  prompt = f\"\"\"Инструкция: Вы — современная гуманитарная онлайн система для обнаружения нарушений прав человека.\n",
        "  Получив публикацию в социальных сетях, ваша задача — классифицировать, содержит ли она упоминания о нарушениях прав человека.\n",
        "  Нарушения прав человека включают (но не ограничиваются): 1) Убийство мирных жителей, 2) Уничтожение гражданских объектов,\n",
        "  3) Изнасилование, пытки, казни. Вы должны возвращать только лейбл \"Да\" или \"Нет\": {text}\"\"\"\n",
        "\n",
        "  # Generate label using Mistral\n",
        "  response = mistral_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label or 'Yes' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using Mistral\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "mistral_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, mistral_labels)\n",
        "precision = precision_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "8a11d475f31f46cc8a8c33da93f0b21b",
            "f3f6368eb9b0468f8d26a696493973fe",
            "a14fc8cabdc14d3ea9c27023fb09737d",
            "ab0777654949491eaf044c11376b3d91",
            "e08d1db672704ccba9eee86b1006adf0",
            "9e0cca83666d4543829dac3b83b5d088",
            "a51c9a610682423eb5d24b7a9f418f50",
            "50b36f786dea4343aebec9aea89784e0",
            "644a97d1cdbf4a29baf98a1a0c19de84",
            "f8a0a2c338d14e6e87e290d300b8ba9d",
            "f24b7a9c8f1f47c39dee6a5eed928fcf"
          ]
        },
        "id": "_Dy6-Eaxct7C",
        "outputId": "ffffce2e-a72a-4ac2-cdbf-7ddf1727a1b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Mistral model with 8-bit quantization...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a11d475f31f46cc8a8c33da93f0b21b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  Mistral7B_One_shot_rus_prompt\n",
            "Accuracy: 0.51\n",
            "Precision: 0.516245487364621\n",
            "Recall: 0.8297872340425532\n",
            "F1 Score: 0.6364985163204748\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 10min 10s (started: 2024-12-08 22:55:21 +00:00)\n"
          ]
        }
      ],
      "source": [
        "### ANOTHER PROMPT VERSION - v2 ###\n",
        "\n",
        "# MISTRAL One shot learning -- RUS prompt, NO EXAMPLES\n",
        "\n",
        "name = 'Mistral7B_One_shot_rus_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "dataset = data  # Replace with your actual dataset\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"  # Update to the Mistral model\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with 8-bit quantization\n",
        "\n",
        "print(\"Loading Mistral model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "# If you want to use the model without 8-bit quantization, uncomment the code below:\n",
        "'''\n",
        "print(\"Loading Mistral model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "mistral_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using Mistral\n",
        "def one_shot_annotate(text):\n",
        "  prompt = f\"\"\"\n",
        "  Читая этот пост в социальных сетях: \"{text}\", можно заметить, что он описывает случай нарушения прав человека. Да или нет?\n",
        "  Классифицируй пост лейблом (только \"Да\" или \"Нет\")\n",
        "  \"\"\"\n",
        "  # Generate label using Mistral\n",
        "  response = mistral_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label or 'Yes' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using Mistral\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "mistral_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, mistral_labels)\n",
        "precision = precision_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4sdWk621XAK"
      },
      "source": [
        "#### Mistral-7B Few Shot, eng prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 828
        },
        "id": "OYYQd6J5xmqI",
        "outputId": "18aee737-838c-45bb-9761-15ee5d0ebb53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Mistral model with 8-bit quantization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-v0.1.\n401 Client Error. (Request ID: Root=1-67573fd2-56c222fd502deb8c0b125ebb;b3a0ecca-993c-4c6a-b325-35dd912b3b61)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-v0.1 is restricted. You must have access to it and be authenticated to access it. Please log in.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    863\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1377\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1297\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    422\u001b[0m             )\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-67573fd2-56c222fd502deb8c0b125ebb;b3a0ecca-993c-4c6a-b325-35dd912b3b61)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-v0.1 is restricted. You must have access to it and be authenticated to access it. Please log in.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-5aa32c2aff5b>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading Mistral model with 8-bit quantization...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    527\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    634\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresolved_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_gated_repo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresolved_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-v0.1.\n401 Client Error. (Request ID: Root=1-67573fd2-56c222fd502deb8c0b125ebb;b3a0ecca-993c-4c6a-b325-35dd912b3b61)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-v0.1 is restricted. You must have access to it and be authenticated to access it. Please log in."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 12.7 s (started: 2024-12-09 19:06:45 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# MISTRAL Few shot learning -- ENG prompt, WITH EXAMPLES\n",
        "\n",
        "name = 'Mistral7B_Few_Shot_eng_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "dataset = data # Replace with your actual dataset\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"  # Update to the Mistral model\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with 8-bit quantization\n",
        "\n",
        "print(\"Loading Mistral model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "# If you want to use the model without 8-bit quantization, uncomment the code below:\n",
        "'''\n",
        "print(\"Loading Mistral model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "\n",
        "mistral_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=10,  # Increase max tokens for better output\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    truncation=True,  # Ensure long prompts are truncated\n",
        "    do_sample=False,\n",
        "    temperature=0.7,  # Lower temperature for more focused results\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "\n",
        "# Define a function for one-shot annotation using Mistral\n",
        "def one_shot_annotate(text):\n",
        "\n",
        "  prompt = f\"\"\"Instruction: You are a humanitarian state-of-the-art Human Rights Violation (HRV) detection system. Given a social media post, your job is to classify\n",
        "  if the post contains HRV mentions. HRV includes (but not limited to): 1) Killing civilians, 2) Destruction of civil objects, 3) Rape, torture, executions.\n",
        "  You need to return just either \"Yes\" or \"No\" label.\n",
        "  Here are some examples:\n",
        "\n",
        "  Example 1\n",
        "  Post: In Mykolaiv, after the arrival at the house, there are already five dead…\n",
        "  Label: Yes\n",
        "\n",
        "  Example 2\n",
        "  Post: Russian aviation struck the Neptune pool in Mariupol where there were pregnant women and mothers with children under 3 years old. Orcs fighting against civilians - a special cauldron in hell is prepared for you. Informer | Subscribe\n",
        "  Label: Yes\n",
        "\n",
        "  Example 3\n",
        "  Post: The first episode is played. There will be others. And more. I will express a personal position. I cannot fail to mention this now. The Ukrainian state in its current configuration with a Nazi political regime will pose a constant, direct, and clear threat to Russia. Therefore, in addition to protecting our people and guarding the country's borders, the goal of our future actions, in my opinion, should be the full dismantling of the political regime in Ukraine.\n",
        "  Label: No\n",
        "\n",
        "  Note that the destruction of military objects (for example, an airbase) is not considered HRV.\n",
        "\n",
        "  Post to label (Yes or No): \"{text}\"\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Generate label using Mistral\n",
        "  response = mistral_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #print(label)\n",
        "\n",
        "  if any(keyword in label for keyword in ['yes', 'да', 'hrv']):\n",
        "      return 'Yes'\n",
        "  elif 'no' in label:\n",
        "      return 'No'\n",
        "  else:\n",
        "      return 'No'  # Default to \"No\" for unexpected outputs\n",
        "\n",
        "\n",
        "# Annotate the dataset using Mistral\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+name+'1000samples.csv')\n",
        "\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "mistral_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, mistral_labels)\n",
        "precision = precision_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIXnt05jKKt8"
      },
      "source": [
        "#### Mistral-7B Few Shot, rus prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "7de447c562ee425997f3ea6089a8e143",
            "89ac018898a04cd1b5a75e1d81e8c157",
            "c55326a1e7b440488ed616d9764ec0e8",
            "a8b5536876564b34a0d2adbabfa37f6b",
            "578a71b25eba48ee84ae5cc8d565c784",
            "e472f02981e540a093006480c1d0e607",
            "a0b9301b9ed74abd9383c72502f35b11",
            "c861810285d64f3c9f182223969bcb9e",
            "30856fa621654a4390dbca5d171343e2",
            "024bc2ee140748d2a25e70198cc9be44",
            "9142f180074645fa99f78d56e36f0ebf"
          ]
        },
        "id": "6PX8UYbkKF54",
        "outputId": "63e9e71c-c79b-4830-856b-9b593ef273e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Mistral model with 8-bit quantization...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7de447c562ee425997f3ea6089a8e143",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:  Mistral7B_Few_Shot_rus_prompt\n",
            "Accuracy: 0.519\n",
            "Precision: 0.5204545454545455\n",
            "Recall: 0.8858800773694391\n",
            "F1 Score: 0.6556907659269864\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 10min 54s (started: 2024-12-09 01:06:31 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# MISTRAL Few shot learning -- ENG prompt, WITH EXAMPLES\n",
        "\n",
        "\n",
        "name = 'Mistral7B_Few_Shot_rus_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "dataset = data  # Replace with your actual dataset\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"  # Update to the Mistral model\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with 8-bit quantization\n",
        "\n",
        "print(\"Loading Mistral model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "# If you want to use the model without 8-bit quantization, uncomment the code below:\n",
        "'''\n",
        "print(\"Loading Mistral model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "mistral_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using Mistral\n",
        "def one_shot_annotate(text):\n",
        "\n",
        "  prompt = f\"\"\"Инструкция: Вы — современная гуманитарная онлайн система для обнаружения нарушений прав человека. Получив публикацию в социальных сетях, ваша задача — классифицировать,\n",
        "  содержит ли она упоминания о нарушениях прав человека. Нарушения включают (но не ограничиваются): 1) Убийство мирных жителей, 2) Уничтожение гражданских объектов, 3) Изнасилование, пытки, казни.\n",
        "  Вы должны возвращать только лейбл \"Да\" или \"Нет\".\n",
        "\n",
        "  Пример 1\n",
        "  Пост: В Николаеве, после прилёта в дом, уже пятеро погибших...\n",
        "  лейбл: Да\n",
        "\n",
        "  Пример 2\n",
        "  Пост: Российская авиация нанесла удар по бассейну «Нептун» в Мариуполе, где находились беременные женщины и матери с детьми до 3 лет. Орки, воюющие против мирных жителей, — для вас в аду приготовлен особый котел. Информатор | Подписывайтесь.\n",
        "  лейбл: Да\n",
        "\n",
        "  Пример 3\n",
        "  Пост: Первый эпизод сыгран. Будут другие. И еще. Я выражу личную позицию. Не могу не упомянуть об этом сейчас. Украинское государство в его нынешней конфигурации с нацистским политическим режимом будет представлять постоянную, прямую и явную угрозу для России. Поэтому, помимо защиты нашего народа и охраны границ страны, целью наших будущих действий, по моему мнению, должно стать полное демонтаж политического режима в Украине.\n",
        "  лейбл: Нет\n",
        "\n",
        "  Учтите, что уничтожение военных объектов (например, авиабазы) не считается нарушением прав человека.\n",
        "\n",
        "  Пост: \"{text}\"\n",
        "  Вы должны предоставить только лейбл (только \"Да\" или \"Нет\"):\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Generate label using Mistral\n",
        "  response = mistral_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using Mistral\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+name+'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "mistral_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, mistral_labels)\n",
        "precision = precision_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, mistral_labels, pos_label=\"Yes\")\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDkpjMhgebUP"
      },
      "source": [
        "### CLAUDE-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wcabBQW1iv6"
      },
      "source": [
        "#### Claude-2 One Shot, eng prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnkDmgNBgWhw",
        "outputId": "db5dd2e7-71dd-4295-8008-2d72e9232ee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.40.0-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.28.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.8.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.27.1)\n",
            "Downloading anthropic-0.40.0-py3-none-any.whl (199 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/199.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.5/199.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.40.0\n",
            "Model:  Claude2_One_Shot_eng_prompt\n",
            "Accuracy: 0.673\n",
            "Precision: 0.786144578313253\n",
            "Recall: 0.504835589941973\n",
            "F1 Score: 0.6148409893992933\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 19min 5s (started: 2024-12-09 01:50:25 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# CLAUDE One Shot Learning -- ENG prompt, NO EXAMPLES\n",
        "\n",
        "# started with $11.07, ended with $8.97 ($2.1 cost!), took 19min 5s\n",
        "!pip install anthropic\n",
        "\n",
        "name = 'Claude2_One_Shot_eng_prompt'\n",
        "\n",
        "import anthropic\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "\n",
        "# Initialize the Claude API client\n",
        "API_KEY = claude_api_key\n",
        "client = anthropic.Client(api_key=API_KEY)\n",
        "\n",
        "# Define a function for one-shot annotation using Claude\n",
        "def one_shot_annotate_claude(text):\n",
        "    prompt = f\"\"\"\n",
        "    You are a detection system for identifying Human Rights Violations (HRV) in text.\n",
        "    Reading the following social media post, decide whether there is a reference to a human rights violation:\n",
        "\n",
        "    Post: \"{text}\"\n",
        "\n",
        "    If the post contains any mention of HRV, respond with \"Yes\". Otherwise, respond with \"No\":\n",
        "    \"\"\"\n",
        "\n",
        "    # Query Claude\n",
        "    response = client.completions.create(\n",
        "        model=\"claude-2\",\n",
        "        prompt=f\"{anthropic.HUMAN_PROMPT} {prompt}{anthropic.AI_PROMPT}\",\n",
        "        max_tokens_to_sample=10,\n",
        "        temperature=0.1  # Low temperature for deterministic output\n",
        "    )\n",
        "\n",
        "    # Extract and clean up the label\n",
        "    label = response.completion.strip().lower()\n",
        "    #print(label)\n",
        "    #return label\n",
        "\n",
        "    # Normalize and classify the output\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label or 'Yes' in label:\n",
        "        return 'Yes'\n",
        "    else:\n",
        "        return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using Claude\n",
        "annotations = [one_shot_annotate_claude(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "claude_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, claude_labels)\n",
        "precision = precision_score(human_labels, claude_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, claude_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, claude_labels, pos_label=\"Yes\")\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeUdhxWJKkN6"
      },
      "source": [
        "#### Claude-2 One Shot, rus prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hijEveUKiKz",
        "outputId": "4b903afa-9995-4ade-fe23-36efcc2f43f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  Claude2_One_Shot_rus_prompt\n",
            "Accuracy: 0.427\n",
            "Precision: 0.4455252918287938\n",
            "Recall: 0.44294003868471954\n",
            "F1 Score: 0.44422890397672166\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\n",
            "time: 19min 11s (started: 2024-12-09 02:14:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# CLAUDE One Shot Learning -- ENG prompt, NO EXAMPLES\n",
        "\n",
        "# started with $8.97, ended with $5.14, ($3.83 cost!), 19min 11s\n",
        "#!pip install anthropic\n",
        "\n",
        "name = 'Claude2_One_Shot_rus_prompt'\n",
        "\n",
        "import anthropic\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "#n = 20\n",
        "dataset = data # Replace with your actual dataset\n",
        "\n",
        "# Initialize the Claude API client\n",
        "API_KEY = claude_api_key\n",
        "client = anthropic.Client(api_key=API_KEY)\n",
        "\n",
        "# Define a function for one-shot annotation using Claude\n",
        "def one_shot_annotate_claude(text):\n",
        "    prompt = fprompt = f\"\"\"Инструкция: Вы — современная гуманитарная онлайн система для обнаружения нарушений прав человека.\n",
        "    Получив публикацию в социальных сетях, ваша задача — классифицировать, содержит ли она упоминания о нарушениях прав человека.\n",
        "    Нарушения прав человека включают (но не ограничиваются): 1) Убийство мирных жителей, 2) Уничтожение гражданских объектов,\n",
        "    3) Изнасилование, пытки, казни. Вы должны возвращать только лейбл \"Да\" или \"Нет\". Пост: {text}\"\"\"\n",
        "\n",
        "\n",
        "    # Query Claude\n",
        "    response = client.completions.create(\n",
        "        model=\"claude-2\",\n",
        "        prompt=f\"{anthropic.HUMAN_PROMPT} {prompt}{anthropic.AI_PROMPT}\",\n",
        "        max_tokens_to_sample=10,\n",
        "        temperature=0.1  # Low temperature for deterministic output\n",
        "    )\n",
        "\n",
        "    # Extract and clean up the label\n",
        "    label = response.completion.strip().lower()\n",
        "    #return label\n",
        "\n",
        "    # Normalize and classify the output\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label:\n",
        "        return 'Yes'\n",
        "    else:\n",
        "        return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using Claude\n",
        "annotations = [one_shot_annotate_claude(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "claude_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, claude_labels)\n",
        "precision = precision_score(human_labels, claude_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, claude_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, claude_labels, pos_label=\"Yes\")\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lW4bQ391mqi"
      },
      "source": [
        "#### Claude-2 Few Shot, eng prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WfRRJQJ_h7ar",
        "outputId": "c7105275-f50f-4420-f528-48c1fe49fd85"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "InternalServerError",
          "evalue": "Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-337206b8a68d>\u001b[0m in \u001b[0;36m<cell line: 88>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# Annotate the dataset using Claude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mone_shot_annotate_claude\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Save annotations to a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-337206b8a68d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# Annotate the dataset using Claude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mone_shot_annotate_claude\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Save annotations to a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-337206b8a68d>\u001b[0m in \u001b[0;36mone_shot_annotate_claude\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m    \u001b[0;31m# Query Claude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     response = client.completions.create(\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"claude-2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{anthropic.HUMAN_PROMPT} {prompt}{anthropic.AI_PROMPT}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/resources/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens_to_sample, model, prompt, metadata, stop_sequences, stream, temperature, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_given\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDEFAULT_TIMEOUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;34m\"/v1/complete\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m         )\n\u001b[0;32m-> 1279\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    957\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1046\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1095\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1046\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1095\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mInternalServerError\u001b[0m: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5min 42s (started: 2024-12-09 23:44:22 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# CLAUDE Few Shot Learning -- ENG prompt, WITH EXAMPLES\n",
        "\n",
        "# started with $12.75,\n",
        "\n",
        "#!pip install anthropic\n",
        "\n",
        "name = 'Claude2_Few_Shot_eng_prompt'\n",
        "\n",
        "import anthropic\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "\n",
        "# Initialize the Claude API client\n",
        "API_KEY = claude_api_key\n",
        "client = anthropic.Client(api_key=API_KEY)\n",
        "\n",
        "# Define a function for one-shot annotation using Claude\n",
        "def one_shot_annotate_claude(text):\n",
        "    prompt = f\"\"\"\n",
        "    You are a detection system for identifying Human Rights Violations (HRV) in text. You will have to annotate posts.\n",
        "     Here are some examples:\n",
        "\n",
        "    Example 1\n",
        "\n",
        "    Post: In Mykolaiv, after the arrival at the house, there are already five dead…\n",
        "    Label: Yes\n",
        "\n",
        "    Example 2\n",
        "\n",
        "    Post: Russian aviation struck the Neptune pool in Mariupol where there were pregnant women and mothers with children under 3 years old. Orcs fighting against civilians - a special cauldron in hell is prepared for you. Informer | Subscribe\n",
        "    Label: Yes\n",
        "\n",
        "    Example 3\n",
        "\n",
        "    Post: The first episode is played. There will be others. And more. I will express a personal position. I cannot fail to mention this now. The Ukrainian state in its current configuration with a Nazi political regime will pose a constant, direct, and clear threat to Russia. Therefore, in addition to protecting our people and guarding the country's borders, the goal of our future actions, in my opinion, should be the full dismantling of the political regime in Ukraine.\n",
        "    Label: No\n",
        "\n",
        "    Note that the destruction of military objects (for example, an airbase) is not considered HRV.\n",
        "\n",
        "    Reading the following social media post, decide whether it clearly describes a human rights violation:\n",
        "\n",
        "    Post: \"{text}\"\n",
        "\n",
        "    If the post contains any mention of HRV, respond with \"Yes\". Otherwise, respond with \"No\".\n",
        "\n",
        "    Label (only \"Yes\" or \"No\"):\n",
        "    \"\"\"\n",
        "\n",
        "   # Query Claude\n",
        "    response = client.completions.create(\n",
        "        model=\"claude-2\",\n",
        "        prompt=f\"{anthropic.HUMAN_PROMPT} {prompt}{anthropic.AI_PROMPT}\",\n",
        "        max_tokens_to_sample=10,\n",
        "        temperature=0.1  # Low temperature for deterministic output\n",
        "    )\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    response = anthropic.Anthropic().messages.create(\n",
        "        model=\"claude-3-5-sonnet-20241022\",\n",
        "        messages=f\" {prompt}\",\n",
        "        max_tokens=10,\n",
        "        temperature=0.1  # Low temperature for deterministic output\n",
        "    )\n",
        "    \"\"\"\n",
        "    # Extract and clean up the label\n",
        "    label = response.completion.strip().lower()\n",
        "    #return label\n",
        "\n",
        "    # Normalize and classify the output\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label or 'Yes' in label:\n",
        "        return 'Yes'\n",
        "    else:\n",
        "        return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using Claude\n",
        "annotations = [one_shot_annotate_claude(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "claude_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, claude_labels)\n",
        "precision = precision_score(human_labels, claude_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, claude_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, claude_labels, pos_label=\"Yes\")\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaNFl66kKzIN"
      },
      "source": [
        "#### Claude-2 Few Shot, rus prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubdxLRKvGJrd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "3b003b05-c6d1-49c6-a1a7-c91f11e8b6f6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "InternalServerError",
          "evalue": "Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-518a7687032a>\u001b[0m in \u001b[0;36m<cell line: 75>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Annotate the dataset using Claude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mone_shot_annotate_claude\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m# Save annotations to a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-518a7687032a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Annotate the dataset using Claude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mone_shot_annotate_claude\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m# Save annotations to a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-518a7687032a>\u001b[0m in \u001b[0;36mone_shot_annotate_claude\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Query Claude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     response = client.completions.create(\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"claude-2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{anthropic.HUMAN_PROMPT} {prompt}{anthropic.AI_PROMPT}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/resources/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens_to_sample, model, prompt, metadata, stop_sequences, stream, temperature, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_given\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDEFAULT_TIMEOUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;34m\"/v1/complete\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m         )\n\u001b[0;32m-> 1279\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    957\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1046\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1095\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1046\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1095\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mInternalServerError\u001b[0m: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.61 s (started: 2024-12-10 00:11:32 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# CLAUDE Few Shot Learning -- ENG prompt, WITH EXAMPLES\n",
        "\n",
        "#!pip install anthropic\n",
        "\n",
        "name = 'Claude2_Few_Shot_rus_prompt'\n",
        "\n",
        "import anthropic\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "\n",
        "# Initialize the Claude API client\n",
        "API_KEY = claude_api_key\n",
        "client = anthropic.Client(api_key=API_KEY)\n",
        "\n",
        "# Define a function for one-shot annotation using Claude\n",
        "def one_shot_annotate_claude(text):\n",
        "    prompt = f\"\"\"\n",
        "    Ваша задача классифировать пост из соцсети на наличине нарушений прав человека.\n",
        "    Примеры:\n",
        "\n",
        "    Пример 1\n",
        "\n",
        "    Пост: В Николаеве, после прилёта в дом, уже пятеро погибших...\n",
        "    лейбл: Да\n",
        "\n",
        "    Пример 2\n",
        "\n",
        "    Пост: Российская авиация нанесла удар по бассейну «Нептун» в Мариуполе, где находились беременные женщины и матери с детьми до 3 лет. Орки, воюющие против мирных жителей, — для вас в аду приготовлен особый котел. Информатор | Подписывайтесь.\n",
        "    лейбл: Да\n",
        "\n",
        "    Пример 3\n",
        "\n",
        "    Пост: Первый эпизод сыгран. Будут другие. И еще. Я выражу личную позицию. Не могу не упомянуть об этом сейчас. Украинское государство в его нынешней конфигурации с нацистским политическим режимом будет представлять постоянную, прямую и явную угрозу для России. Поэтому, помимо защиты нашего народа и охраны границ страны, целью наших будущих действий, по моему мнению, должно стать полное демонтаж политического режима в Украине.\n",
        "    лейбл: Нет\n",
        "\n",
        "    Учтите, что уничтожение военных объектов (например, авиабазы) не считается нарушением прав человека.\n",
        "\n",
        "    Вы должны предоставить только лейбл.\n",
        "\n",
        "    Читая этот пост в социальных сетях: \"{text}\", можно заметить, что он описывает случай нарушения прав человека. Верно?\n",
        "    Классифицируй пост лейблом (только \"Да\" или \"Нет\")\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Query Claude\n",
        "    response = client.completions.create(\n",
        "        model=\"claude-2\",\n",
        "        prompt=f\"{anthropic.HUMAN_PROMPT} {prompt}{anthropic.AI_PROMPT}\",\n",
        "        max_tokens_to_sample=10,\n",
        "        temperature=0.1  # Low temperature for deterministic output\n",
        "    )\n",
        "\n",
        "    # Extract and clean up the label\n",
        "    label = response.completion.strip().lower()\n",
        "    #return label\n",
        "\n",
        "    # Normalize and classify the output\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label:\n",
        "        return 'Yes'\n",
        "    else:\n",
        "        return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using Claude\n",
        "annotations = [one_shot_annotate_claude(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/drive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "claude_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, claude_labels)\n",
        "precision = precision_score(human_labels, claude_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, claude_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, claude_labels, pos_label=\"Yes\")\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ABLATION STUDY**"
      ],
      "metadata": {
        "id": "narReglY3T4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### DATA"
      ],
      "metadata": {
        "id": "O-YJ4jqn6b7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data\n",
        "\n",
        "human_agreement = human_annotations[human_annotations['HRV 1']==human_annotations['HRV 2']]\n",
        "human_disagreement = human_annotations[human_annotations['HRV 1']!=human_annotations['HRV 2']]\n",
        "human_annotations = pd.concat([human_agreement, human_disagreement], ignore_index=True) # I want to have forst 85 row with perfect agreement first, and then 15 with disagreement\n",
        "len(human_disagreement)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIq1pzCu305U",
        "outputId": "4b9b12de-37d3-4eee-9afa-721f13ad637e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "184"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.96 ms (started: 2024-12-09 19:39:26 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# labels\n",
        "\n",
        "data_agreement = human_agreement['Post'].to_list()\n",
        "human_agreement_annotations = human_agreement['Adjudicated HRV label'].to_list()\n",
        "\n",
        "data_disagreement = human_disagreement['Post'].tolist()\n",
        "human_disagreement_annotations = human_disagreement['Adjudicated HRV label'].to_list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4zOjijl5DP_",
        "outputId": "58329511-c1e0-4755-b270-d7b9d381d598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 873 µs (started: 2024-12-09 19:39:26 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up new results doc to write to\n",
        "\n",
        "results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_agreement_set.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3IV2AeH6vD6",
        "outputId": "3c0ab28b-fcd6-4f58-e986-796178943899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 389 µs (started: 2024-12-09 19:39:26 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Agreement experiment first"
      ],
      "metadata": {
        "id": "RkWNwumm6X9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_agreement\n",
        "human_labels = human_agreement_annotations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSkZYSzU6Vc2",
        "outputId": "d631ad0f-634a-432a-99ae-90970406b5cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 423 µs (started: 2024-12-09 22:20:37 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpt4Fno-4Fax"
      },
      "source": [
        "##### LLAMA 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9yeJKKx4Fay"
      },
      "source": [
        "###### Accessing HuggingFace Gated Repos (Needed for LLAMA and Mistral)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38068246-6437-4906-a0bf-85c78eb3bba3",
        "id": "8PSVzew44Fay"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 104 ms (started: 2024-12-09 19:39:38 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!git config --global credential.helper store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d035268-9122-450e-b36a-a8a04f9c72fd",
        "id": "FWV36law4Fay"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `LLM annotations study` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `LLM annotations study`\n",
            "time: 13.1 s (started: 2024-12-09 19:39:40 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud18xI9o4Fay"
      },
      "source": [
        "###### Setting up 8-bit quantization (needed for LLAMA and Mistral)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a99d598-6943-4783-ab3c-0a82243f15ba",
        "id": "q4xb5Ybt4Fay"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
            "time: 2.51 s (started: 2024-12-09 19:37:31 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjkJCl4u4Fay"
      },
      "outputs": [],
      "source": [
        "!pip uninstall bitsandbytes transformers\n",
        "!pip install transformers bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST5jsNrp4Fay"
      },
      "source": [
        "##### Llama 3 One shot, eng prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b935418d-db58-4141-948c-1fd960a46cac",
        "id": "MblAfrGG4Faz"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  LLama3_One_Shot_eng_prompt\n",
            "Accuracy: 0.5318627450980392\n",
            "Precision: 0.533083645443196\n",
            "Recall: 0.9816091954022989\n",
            "F1 Score: 0.6909385113268608\n",
            "time: 1.06 ms (started: 2024-12-09 19:47:04 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Llama 3 One shot learning -- ENG prompt, NO EXAMPLES\n",
        "\n",
        "name = 'LLama3_One_Shot_eng_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with the new quantization config\n",
        "\n",
        "print(\"Loading model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "''' # if I want to use the model withought 8-bit quantization\n",
        "print(\"Loading model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using LLaMA\n",
        "def one_shot_annotate(text):\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Does this social media post describe a human rights violation?\n",
        "  Post: \"{text}\".\n",
        "  Respond with \"Yes\" or \"No\" only.\n",
        "  Label: \"\"\"\n",
        "\n",
        "\n",
        "  # Generate label using LLaMA\n",
        "  response = llama_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using LLaMA\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "llama_labels = annotations\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, llama_labels)\n",
        "precision = precision_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "#results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n",
        "\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mqd_8Sk4Faz"
      },
      "source": [
        "##### Llama 3 One shot, rus prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b4eb341-5292-4e3a-89d2-3df7ac61fb26",
        "id": "CWa-sCoM4Faz"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model with 8-bit quantization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  Llama3_One_shot_rus_prompt\n",
            "Accuracy: 0.48161764705882354\n",
            "Precision: 0.5454545454545454\n",
            "Recall: 0.16551724137931034\n",
            "F1 Score: 0.25396825396825395\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_agreement_set.txt\n",
            "time: 1min 34s (started: 2024-12-09 22:21:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Llama 3 One shot learning -- ENG prompt, NO EXAMPLES\n",
        "\n",
        "name = 'Llama3_One_shot_rus_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with the new quantization config\n",
        "\n",
        "print(\"Loading model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "''' # if I want to use the model withought 8-bit quantization\n",
        "print(\"Loading model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using LLaMA\n",
        "def one_shot_annotate(text):\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Ваша задача — определить, содержит ли пост из соцсети упоминания о нарушениях прав человека.\n",
        "\n",
        "  Пост: \"{text}\"\n",
        "  Предоставьте лейбл (только \"Yes\" или \"No\")\n",
        "  \"\"\"\n",
        "\n",
        "  # Generate label using LLaMA\n",
        "  response = llama_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using LLaMA\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples_agreement.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "llama_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, llama_labels)\n",
        "precision = precision_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "#results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz4w4CZL4Fa0"
      },
      "source": [
        "####Llama 3 Few shot, eng prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b17fde-885a-49d1-aa83-66ea692b0677",
        "id": "U-PauoC24Fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model with 8-bit quantization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  Llama3_Few_shot_eng_prompt\n",
            "Accuracy: 0.5428921568627451\n",
            "Precision: 0.5389447236180904\n",
            "Recall: 0.9862068965517241\n",
            "F1 Score: 0.6969943135662063\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_agreement_set.txt\n",
            "time: 2min 58s (started: 2024-12-09 22:27:56 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Llama 3 Few shot learning -- ENG prompt, WITH EXAMPLES\n",
        "\n",
        "name = 'Llama3_Few_shot_eng_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "#from datasets import load_dataset\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with the new quantization config\n",
        "\n",
        "print(\"Loading model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "''' # if I want to use the model withought 8-bit quantization\n",
        "print(\"Loading model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using LLaMA\n",
        "def one_shot_annotate(text):\n",
        "  prompt = f\"\"\"Instruction: You are a humanitarian state-of-the-art Human Rights Violation (HRV) detection system. Given a social media post, your job is to classify\n",
        "  if the post contains HRV mentions. HRV includes (but not limited to): 1) Killing civilians, 2) Destruction of civil objects, 3) Rape, torture, executions. You need to return just either \"Yes\" or \"No\" label.\n",
        "  Here are some examples:\n",
        "\n",
        "  Example 1\n",
        "  Post: In Mykolaiv, after the arrival at the house, there are already five dead…\n",
        "  Label: Yes\n",
        "\n",
        "  Example 2\n",
        "  Post: Russian aviation struck the Neptune pool in Mariupol where there were pregnant women and mothers with children under 3 years old. Orcs fighting against civilians - a special cauldron in hell is prepared for you. Informer | Subscribe\n",
        "  Label: Yes\n",
        "\n",
        "  Example 3\n",
        "  Post: The first episode is played. There will be others. And more. I will express a personal position. I cannot fail to mention this now. The Ukrainian state in its current configuration with a Nazi political regime will pose a constant, direct, and clear threat to Russia. Therefore, in addition to protecting our people and guarding the country's borders, the goal of our future actions, in my opinion, should be the full dismantling of the political regime in Ukraine.\n",
        "  Label: No\n",
        "\n",
        "  Note that the destruction of military objects (for example, an airbase) is not considered HRV.\n",
        "  Post: {text}\n",
        "  Label (only \"Yes\" or \"No\"):\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Generate label using LLaMA\n",
        "  response = llama_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using LLaMA\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "llama_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, llama_labels)\n",
        "precision = precision_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "\n",
        "\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "#results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLKtL-fV4Fa0"
      },
      "source": [
        "#### Llama 3 Few shot, rus prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd418b0-0283-4f0f-f2f6-5380f216a653",
        "id": "j0OTpEH64Fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model with 8-bit quantization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  Llama3_Few_shot_rus_prompt\n",
            "Accuracy: 0.5318627450980392\n",
            "Precision: 0.5325153374233129\n",
            "Recall: 0.9977011494252873\n",
            "F1 Score: 0.6944\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_agreement_set.txt\n",
            "time: 3min 1s (started: 2024-12-09 22:31:06 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Llama 3 Few shot learning -- RUS prompt, WITH EXAMPLES\n",
        "\n",
        "name = 'Llama3_Few_shot_rus_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "#from datasets import load_dataset\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with the new quantization config\n",
        "\n",
        "print(\"Loading model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "''' # if I want to use the model withought 8-bit quantization\n",
        "print(\"Loading model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using LLaMA\n",
        "def one_shot_annotate(text):\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Инструкция: Вы — гуманитарная система для обнаружения нарушений прав человека. Ваша задача — классифицировать, содержит ли публикация в социальных сетях упоминания о нарушениях прав человека.\n",
        "\n",
        "  Что считается нарушением прав человека:\n",
        "  - Убийство мирных жителей\n",
        "  - Уничтожение гражданских объектов\n",
        "  - Изнасилование, пытки, казни\n",
        "\n",
        "  Пример 1:\n",
        "  Пост: В Николаеве, после прилёта в дом, уже пятеро погибших...\n",
        "  Лейбл: Да\n",
        "\n",
        "  Пример 2:\n",
        "  Пост: Российская авиация нанесла удар по бассейну «Нептун» в Мариуполе, где находились беременные женщины и матери с детьми до 3 лет. Орки, воюющие против мирных жителей, — для вас в аду приготовлен особый котел. Информатор | Подписывайтесь.\n",
        "  Лейбл: Да\n",
        "\n",
        "  Пример 3:\n",
        "  Пост: Первый эпизод сыгран. Будут другие. И еще. Я выражу личную позицию. Не могу не упомянуть об этом сейчас. Украинское государство в его нынешней конфигурации с нацистским политическим режимом будет представлять постоянную, прямую и явную угрозу для России. Поэтому, помимо защиты нашего народа и охраны границ страны, целью наших будущих действий, по моему мнению, должно стать полное демонтаж политического режима в Украине.\n",
        "  Лейбл: Нет\n",
        "\n",
        "  Теперь классифицируйте следующий пост:\n",
        "  Пост: \"{text}\"\n",
        "\n",
        "  Ответьте только \"Да\" или \"Нет\":\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "  # Generate label using LLaMA\n",
        "  response = llama_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label or 'Yes' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using LLaMA\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "llama_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, llama_labels)\n",
        "precision = precision_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "#results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GPT4o"
      ],
      "metadata": {
        "id": "vtEgizSt54og"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zd38pGa158pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvrdqcQX6CMq"
      },
      "source": [
        "####GPT 4o Few Shot, eng prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "614afe0d-0d79-4eed-b263-92cf07c0e065",
        "id": "JA49MITK6CMq"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  GPT_4o_Few_Shot_eng_prompt\n",
            "Sample size:  1000\n",
            "Accuracy: 0.8602941176470589\n",
            "Precision: 0.8962962962962963\n",
            "Recall: 0.8344827586206897\n",
            "F1 Score: 0.8642857142857143\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_agreement_set.txt\n",
            "time: 6min 9s (started: 2024-12-09 22:36:35 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#  GPT 4o Few Shot Learning -- ENG instructions, WITH EXAMPLES\n",
        "\n",
        "# started with $11.76, ended with $10.58, took 7min 16s\n",
        "\n",
        "import openai\n",
        "\n",
        "name = 'GPT_4o_Few_Shot_eng_prompt'\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "def label_with_chatgpt(prompt, data):\n",
        "    responses = []\n",
        "    for item in data:\n",
        "        full_prompt = f\"{prompt}\\n\\nData: {item}\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "            max_tokens=50\n",
        "        )\n",
        "        label = response.choices[0].message['content'].strip()\n",
        "        responses.append(label)\n",
        "    return responses\n",
        "\n",
        "\n",
        "### ALL ENGLIGH PROMPT\n",
        "prompt = \"\"\"Instruction: You are a humanitarian state-of-the-art Human Rights Violation (HRV) detection system. Given a social media post, your job is to classify\n",
        "if the post contains HRV mentions. HRV includes (but not limited to): 1) Killing civilians, 2) Destruction of civil objects, 3) Rape, torture, executions. You need to return just either \"Yes\" or \"No\" label.\n",
        "Here are some examples:\n",
        "\n",
        "Example 1\n",
        "\n",
        "Post: In Mykolaiv, after the arrival at the house, there are already five dead…\n",
        "Label: Yes\n",
        "\n",
        "Example 2\n",
        "\n",
        "Post: Russian aviation struck the Neptune pool in Mariupol where there were pregnant women and mothers with children under 3 years old. Orcs fighting against civilians - a special cauldron in hell is prepared for you. Informer | Subscribe\n",
        "Label: Yes\n",
        "\n",
        "Example 3\n",
        "\n",
        "Post: The first episode is played. There will be others. And more. I will express a personal position. I cannot fail to mention this now. The Ukrainian state in its current configuration with a Nazi political regime will pose a constant, direct, and clear threat to Russia. Therefore, in addition to protecting our people and guarding the country's borders, the goal of our future actions, in my opinion, should be the full dismantling of the political regime in Ukraine.\n",
        "Label: No\n",
        "\n",
        "Note that the destruction of military objects (for example, an airbase) is not considered HRV.\n",
        "\n",
        "You need to putput label only.\"\"\"\n",
        "\n",
        "\n",
        "#data = data['Post'].to_list()\n",
        "\n",
        "# Get labels\n",
        "labels_eng_prompt = label_with_chatgpt(prompt, data)\n",
        "\n",
        "def normalize_label(labels):\n",
        "  out = []\n",
        "  for label in labels:\n",
        "    label = label.lower()  # Normalize to lowercase\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label:  # Check for relevant terms\n",
        "      out.append('Yes')\n",
        "    else:\n",
        "      out.append('No')\n",
        "  return out\n",
        "\n",
        "# Apply the normalization function to the list\n",
        "labels_eng_prompt = normalize_label(labels_eng_prompt)\n",
        "\n",
        "labels_eng_prompt = pd.DataFrame(labels_eng_prompt)\n",
        "\n",
        "#labels_eng_prompt.to_csv('/content/gdrive/My Drive/HRV NLP Project X/Data/Data [January 2024]/gpt_labels_eng_prompt_hrv.csv')\n",
        "\n",
        "\n",
        "# evaluation\n",
        "\n",
        "# encoding\n",
        "#labels_eng_prompt['labels'] = labels_eng_prompt[0].replace({'Yes': 1, 'No': 0})\n",
        "gpt_labels = labels_eng_prompt[0].to_list()\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_true = human_labels\n",
        "y_pred = gpt_labels\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "print(\"Model: \", name)\n",
        "print(\"Sample size: \", n)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "recall = recall_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "f1 = f1_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': gpt_labels,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "#results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gPJLY4H6CMq"
      },
      "source": [
        "####GPT 4o Few Shot, rus prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c768b84-42df-471d-cc6a-f1138b44df04",
        "id": "n-2ggWTD6CMq"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  GPT_4o_Few_Shot_rus_prompt\n",
            "Sample size:  816\n",
            "Accuracy: 0.8578431372549019\n",
            "Precision: 0.9164490861618799\n",
            "Recall: 0.8068965517241379\n",
            "F1 Score: 0.8581907090464548\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_agreement_set.txt\n",
            "time: 5min 34s (started: 2024-12-09 22:44:45 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#  GPT 4o Few Shot Learning -- ENG instructions, WITH EXAMPLES\n",
        "\n",
        "# started with $10.58, ended with $9.28, took 5min 40s\n",
        "\n",
        "import openai\n",
        "\n",
        "name = 'GPT_4o_Few_Shot_rus_prompt'\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "def label_with_chatgpt(prompt, data):\n",
        "    responses = []\n",
        "    for item in data:\n",
        "        full_prompt = f\"{prompt}\\n\\nData: {item}\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "            max_tokens=50\n",
        "        )\n",
        "        label = response.choices[0].message['content'].strip()\n",
        "        responses.append(label)\n",
        "    return responses\n",
        "\n",
        "\n",
        "### ALL RUSSIAN PROMPT\n",
        "\n",
        "prompt = \"\"\"Инструкция: Вы — современная гуманитарная онлайн система для обнаружения нарушений прав человека. Получив публикацию в социальных сетях, ваша задача — классифицировать,\n",
        "содержит ли она упоминания о нарушениях прав человека. Нарушения включают (но не ограничиваются): 1) Убийство мирных жителей, 2) Уничтожение гражданских объектов, 3) Изнасилование, пытки, казни.\n",
        "Вы должны возвращать только лейбл \"Да\" или \"Нет\".\n",
        "\n",
        "Примеры:\n",
        "\n",
        "Пример 1\n",
        "\n",
        "Пост: В Николаеве, после прилёта в дом, уже пятеро погибших...\n",
        "лейбл: Да\n",
        "\n",
        "Пример 2\n",
        "\n",
        "Пост: Российская авиация нанесла удар по бассейну «Нептун» в Мариуполе, где находились беременные женщины и матери с детьми до 3 лет. Орки, воюющие против мирных жителей, — для вас в аду приготовлен особый котел. Информатор | Подписывайтесь.\n",
        "лейбл: Да\n",
        "\n",
        "Пример 3\n",
        "\n",
        "Пост: Первый эпизод сыгран. Будут другие. И еще. Я выражу личную позицию. Не могу не упомянуть об этом сейчас. Украинское государство в его нынешней конфигурации с нацистским политическим режимом будет представлять постоянную, прямую и явную угрозу для России. Поэтому, помимо защиты нашего народа и охраны границ страны, целью наших будущих действий, по моему мнению, должно стать полное демонтаж политического режима в Украине.\n",
        "лейбл: Нет\n",
        "\n",
        "Учтите, что уничтожение военных объектов (например, авиабазы) не считается нарушением прав человека.\n",
        "\n",
        "Вы должны предоставить только лейбл.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Get labels\n",
        "labels_eng_prompt = label_with_chatgpt(prompt, data)\n",
        "\n",
        "def normalize_label(labels):\n",
        "  out = []\n",
        "  for label in labels:\n",
        "    label = label.lower()  # Normalize to lowercase\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label:  # Check for relevant terms\n",
        "      out.append('Yes')\n",
        "    else:\n",
        "      out.append('No')\n",
        "  return out\n",
        "\n",
        "# Apply the normalization function to the list\n",
        "labels_eng_prompt = normalize_label(labels_eng_prompt)\n",
        "\n",
        "labels_eng_prompt = pd.DataFrame(labels_eng_prompt)\n",
        "\n",
        "#labels_eng_prompt.to_csv('/content/gdrive/My Drive/HRV NLP Project X/Data/Data [January 2024]/gpt_labels_eng_prompt_hrv.csv')\n",
        "\n",
        "\n",
        "# evaluation\n",
        "\n",
        "# encoding\n",
        "#labels_eng_prompt['labels'] = labels_eng_prompt[0].replace({'Yes': 1, 'No': 0})\n",
        "gpt_labels = labels_eng_prompt[0].to_list()\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_true = human_labels\n",
        "y_pred = gpt_labels\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "print(\"Model: \", name)\n",
        "print(\"Sample size: \", len(data))\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "recall = recall_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "f1 = f1_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': gpt_labels,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "#results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {len(data)}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ5a6J6V6CMq"
      },
      "source": [
        "####GPT 4o One Shot, eng prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bd26d09-0a64-4a6b-edc9-e6a6511b23d4",
        "id": "vdVVJSI36CMr"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  GPT_4o_One_Shot_eng_prompt\n",
            "Sample size:  1000\n",
            "Accuracy: 0.8468137254901961\n",
            "Precision: 0.8163265306122449\n",
            "Recall: 0.9195402298850575\n",
            "F1 Score: 0.8648648648648649\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_agreement_set.txt\n",
            "time: 6min 28s (started: 2024-12-09 22:58:29 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#  GPT 4o One Shot Learning -- ENG instructions, NO EXAMPLES\n",
        "\n",
        "# started with $9.28, ended with $8.72\n",
        "import openai\n",
        "\n",
        "name = 'GPT_4o_One_Shot_eng_prompt'\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "def label_with_chatgpt(prompt, data):\n",
        "    responses = []\n",
        "    for item in data:\n",
        "        full_prompt = f\"{prompt}\\n\\nData: {item}\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "            max_tokens=50\n",
        "        )\n",
        "        label = response.choices[0].message['content'].strip()\n",
        "        responses.append(label)\n",
        "    return responses\n",
        "\n",
        "\n",
        "### ALL ENGLIGH PROMPT\n",
        "prompt = \"\"\"Instruction: You are a humanitarian state-of-the-art Human Rights Violation (HRV) detection system. Given a social media post, your job is to classify\n",
        "if the post contains HRV mentions. HRV includes (but not limited to): 1) Killing civilians, 2) Destruction of civil objects, 3) Rape, torture, executions. You need to return just either \"Yes\" or \"No\" label. \"\"\"\n",
        "\n",
        "\n",
        "# Get labels\n",
        "labels_eng_prompt = label_with_chatgpt(prompt, data)\n",
        "\n",
        "def normalize_label(labels):\n",
        "  out = []\n",
        "  for label in labels:\n",
        "    label = label.lower()  # Normalize to lowercase\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label:  # Check for relevant terms\n",
        "      out.append('Yes')\n",
        "    else:\n",
        "      out.append('No')\n",
        "  return out\n",
        "\n",
        "# Apply the normalization function to the list\n",
        "labels_eng_prompt = normalize_label(labels_eng_prompt)\n",
        "\n",
        "labels_eng_prompt = pd.DataFrame(labels_eng_prompt)\n",
        "\n",
        "#labels_eng_prompt.to_csv('/content/gdrive/My Drive/HRV NLP Project X/Data/Data [January 2024]/gpt_labels_eng_prompt_hrv.csv')\n",
        "\n",
        "\n",
        "# evaluation\n",
        "\n",
        "# encoding\n",
        "#labels_eng_prompt['labels'] = labels_eng_prompt[0].replace({'Yes': 1, 'No': 0})\n",
        "gpt_labels = labels_eng_prompt[0].to_list()\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_true = human_labels\n",
        "y_pred = gpt_labels\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "print(\"Model: \", name)\n",
        "print(\"Sample size: \", n)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "recall = recall_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "f1 = f1_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': gpt_labels,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "#results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuJ4-MBW6CMr"
      },
      "source": [
        "####GPT 4o One Shot, rus prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "847428c8-4ddf-441f-a3b2-63656cf5c173",
        "id": "A_zxzjO26CMr"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  GPT_4o_One_Shot_rus_prompt\n",
            "Sample size:  1000\n",
            "Accuracy: 0.8419117647058824\n",
            "Precision: 0.8200836820083682\n",
            "Recall: 0.9011494252873563\n",
            "F1 Score: 0.8587075575027382\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_agreement_set.txt\n",
            "time: 7min 35s (started: 2024-12-09 23:06:24 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#  GPT 4o One Shot Learning -- ENG instructions, NO EXAMPLES\n",
        "\n",
        "# started with $8.72, ended with $8.10, took 8min 27s\n",
        "import openai\n",
        "\n",
        "name = 'GPT_4o_One_Shot_rus_prompt'\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "def label_with_chatgpt(prompt, data):\n",
        "    responses = []\n",
        "    for item in data:\n",
        "        full_prompt = f\"{prompt}\\n\\nData: {item}\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "            max_tokens=50\n",
        "        )\n",
        "        label = response.choices[0].message['content'].strip()\n",
        "        responses.append(label)\n",
        "    return responses\n",
        "\n",
        "\n",
        "### ALL RUSSIAN PROMPT\n",
        "prompt = \"\"\"Инструкция: Вы — современная гуманитарная онлайн система для обнаружения нарушений прав человека.\n",
        "Получив публикацию в социальных сетях, ваша задача — классифицировать, содержит ли она упоминания о нарушениях прав человека.\n",
        "Нарушения прав человека включают (но не ограничиваются): 1) Убийство мирных жителей, 2) Уничтожение гражданских объектов,\n",
        "3) Изнасилование, пытки, казни. Вы должны возвращать только лейбл \"Да\" или \"Нет\" \"\"\"\n",
        "\n",
        "\n",
        "# Get labels\n",
        "labels_eng_prompt = label_with_chatgpt(prompt, data)\n",
        "\n",
        "def normalize_label(labels):\n",
        "  out = []\n",
        "  for label in labels:\n",
        "    label = label.lower()  # Normalize to lowercase\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label:  # Check for relevant terms\n",
        "      out.append('Yes')\n",
        "    else:\n",
        "      out.append('No')\n",
        "  return out\n",
        "\n",
        "# Apply the normalization function to the list\n",
        "labels_eng_prompt = normalize_label(labels_eng_prompt)\n",
        "\n",
        "labels_eng_prompt = pd.DataFrame(labels_eng_prompt)\n",
        "\n",
        "# evaluation\n",
        "\n",
        "# encoding\n",
        "#labels_eng_prompt['labels'] = labels_eng_prompt[0].replace({'Yes': 1, 'No': 0})\n",
        "gpt_labels = labels_eng_prompt[0].to_list()\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_true = human_labels\n",
        "y_pred = gpt_labels\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "print(\"Model: \", name)\n",
        "print(\"Sample size: \", n)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "recall = recall_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "f1 = f1_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': gpt_labels,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "#results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Disagreement test"
      ],
      "metadata": {
        "id": "WtN6k7bMtFD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_disagreement\n",
        "human_labels = human_disagreement_annotations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1fb61f-f5a9-425a-9aee-4f1d98ed9919",
        "id": "p3uK8hmFtNYm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 395 µs (started: 2024-12-09 23:17:39 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0TcykiKtNYn"
      },
      "source": [
        "##### LLAMA 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky5x9k_otNYn"
      },
      "source": [
        "###### Accessing HuggingFace Gated Repos (Needed for LLAMA and Mistral)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38068246-6437-4906-a0bf-85c78eb3bba3",
        "id": "iJmOIgUptNYn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 104 ms (started: 2024-12-09 19:39:38 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!git config --global credential.helper store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d035268-9122-450e-b36a-a8a04f9c72fd",
        "id": "fgKbMazHtNYn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `LLM annotations study` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `LLM annotations study`\n",
            "time: 13.1 s (started: 2024-12-09 19:39:40 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OfKPDY2tNYn"
      },
      "source": [
        "###### Setting up 8-bit quantization (needed for LLAMA and Mistral)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a99d598-6943-4783-ab3c-0a82243f15ba",
        "id": "17utz4PVtNYn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
            "time: 2.51 s (started: 2024-12-09 19:37:31 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UsvudWBtNYn"
      },
      "outputs": [],
      "source": [
        "!pip uninstall bitsandbytes transformers\n",
        "!pip install transformers bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq8DuSg_tNYn"
      },
      "source": [
        "##### Llama 3 One shot, eng prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1971665a-becc-4ea2-95e6-7e90779fe7e3",
        "id": "JCThSrYBtNYn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model with 8-bit quantization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  LLama3_One_Shot_eng_prompt\n",
            "Accuracy: 0.44565217391304346\n",
            "Precision: 0.4425287356321839\n",
            "Recall: 0.9390243902439024\n",
            "F1 Score: 0.6015625\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_agreement_set.txt\n",
            "Model:  LLama3_One_Shot_eng_prompt\n",
            "Accuracy: 0.44565217391304346\n",
            "Precision: 0.4425287356321839\n",
            "Recall: 0.9390243902439024\n",
            "F1 Score: 0.6015625\n",
            "time: 41.5 s (started: 2024-12-09 23:17:52 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Llama 3 One shot learning -- ENG prompt, NO EXAMPLES\n",
        "\n",
        "name = 'LLama3_One_Shot_eng_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with the new quantization config\n",
        "\n",
        "print(\"Loading model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "''' # if I want to use the model withought 8-bit quantization\n",
        "print(\"Loading model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using LLaMA\n",
        "def one_shot_annotate(text):\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Does this social media post describe a human rights violation?\n",
        "  Post: \"{text}\".\n",
        "  Respond with \"Yes\" or \"No\" only.\n",
        "  Label: \"\"\"\n",
        "\n",
        "\n",
        "  # Generate label using LLaMA\n",
        "  response = llama_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using LLaMA\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "llama_labels = annotations\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, llama_labels)\n",
        "precision = precision_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xklR_8HstNYn"
      },
      "source": [
        "##### Llama 3 One shot, rus prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84e8bd69-6373-425c-a052-3f1d961dedb3",
        "id": "PHIGce2xtNYn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model with 8-bit quantization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  Llama3_One_shot_rus_prompt\n",
            "Accuracy: 0.5760869565217391\n",
            "Precision: 0.5714285714285714\n",
            "Recall: 0.1951219512195122\n",
            "F1 Score: 0.2909090909090909\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_agreement_set.txt\n",
            "time: 23.4 s (started: 2024-12-09 23:18:43 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Llama 3 One shot learning -- ENG prompt, NO EXAMPLES\n",
        "\n",
        "name = 'Llama3_One_shot_rus_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with the new quantization config\n",
        "\n",
        "print(\"Loading model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "''' # if I want to use the model withought 8-bit quantization\n",
        "print(\"Loading model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using LLaMA\n",
        "def one_shot_annotate(text):\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Ваша задача — определить, содержит ли пост из соцсети упоминания о нарушениях прав человека.\n",
        "\n",
        "  Пост: \"{text}\"\n",
        "  Предоставьте лейбл (только \"Yes\" или \"No\")\n",
        "  \"\"\"\n",
        "\n",
        "  # Generate label using LLaMA\n",
        "  response = llama_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using LLaMA\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples_agreement.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "llama_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, llama_labels)\n",
        "precision = precision_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "#results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TahvEZS9tNYn"
      },
      "source": [
        "####Llama 3 Few shot, eng prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "611755ea-88ea-47f7-c85b-05341c3a2294",
        "id": "NBBUWotYtNYn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model with 8-bit quantization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  Llama3_Few_shot_eng_prompt\n",
            "Accuracy: 0.45108695652173914\n",
            "Precision: 0.44508670520231214\n",
            "Recall: 0.9390243902439024\n",
            "F1 Score: 0.6039215686274509\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_agreement_set.txt\n",
            "time: 43.3 s (started: 2024-12-09 23:20:03 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Llama 3 Few shot learning -- ENG prompt, WITH EXAMPLES\n",
        "\n",
        "name = 'Llama3_Few_shot_eng_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "#from datasets import load_dataset\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with the new quantization config\n",
        "\n",
        "print(\"Loading model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "''' # if I want to use the model withought 8-bit quantization\n",
        "print(\"Loading model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using LLaMA\n",
        "def one_shot_annotate(text):\n",
        "  prompt = f\"\"\"Instruction: You are a humanitarian state-of-the-art Human Rights Violation (HRV) detection system. Given a social media post, your job is to classify\n",
        "  if the post contains HRV mentions. HRV includes (but not limited to): 1) Killing civilians, 2) Destruction of civil objects, 3) Rape, torture, executions. You need to return just either \"Yes\" or \"No\" label.\n",
        "  Here are some examples:\n",
        "\n",
        "  Example 1\n",
        "  Post: In Mykolaiv, after the arrival at the house, there are already five dead…\n",
        "  Label: Yes\n",
        "\n",
        "  Example 2\n",
        "  Post: Russian aviation struck the Neptune pool in Mariupol where there were pregnant women and mothers with children under 3 years old. Orcs fighting against civilians - a special cauldron in hell is prepared for you. Informer | Subscribe\n",
        "  Label: Yes\n",
        "\n",
        "  Example 3\n",
        "  Post: The first episode is played. There will be others. And more. I will express a personal position. I cannot fail to mention this now. The Ukrainian state in its current configuration with a Nazi political regime will pose a constant, direct, and clear threat to Russia. Therefore, in addition to protecting our people and guarding the country's borders, the goal of our future actions, in my opinion, should be the full dismantling of the political regime in Ukraine.\n",
        "  Label: No\n",
        "\n",
        "  Note that the destruction of military objects (for example, an airbase) is not considered HRV.\n",
        "  Post: {text}\n",
        "  Label (only \"Yes\" or \"No\"):\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Generate label using LLaMA\n",
        "  response = llama_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using LLaMA\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "llama_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, llama_labels)\n",
        "precision = precision_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "\n",
        "\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "#results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQjbbJ-GtNYo"
      },
      "source": [
        "#### Llama 3 Few shot, rus prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a2aa08b-bb08-494a-87e2-ae5b4cfb8c54",
        "id": "hNE-Ap-stNYo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model with 8-bit quantization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  Llama3_Few_shot_rus_prompt\n",
            "Accuracy: 0.44565217391304346\n",
            "Precision: 0.44565217391304346\n",
            "Recall: 1.0\n",
            "F1 Score: 0.6165413533834586\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_agreement_set.txt\n",
            "time: 43.9 s (started: 2024-12-09 23:22:49 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Llama 3 Few shot learning -- RUS prompt, WITH EXAMPLES\n",
        "\n",
        "name = 'Llama3_Few_shot_rus_prompt'\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "#from datasets import load_dataset\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "dataset = data  # Replace with your actual dataset\n",
        "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Configure the BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with the new quantization config\n",
        "\n",
        "print(\"Loading model with 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "''' # if I want to use the model withought 8-bit quantization\n",
        "print(\"Loading model WITHOUT 8-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "'''\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad_token_id to eos_token_id to avoid warnings\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "# Create the pipeline with greedy decoding for consistency\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3,\n",
        "    num_return_sequences=1,\n",
        "    return_full_text=False,\n",
        "    do_sample=False,  # Greedy decoding\n",
        "    temperature=0.9,  # Ensure deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Define a function for one-shot annotation using LLaMA\n",
        "def one_shot_annotate(text):\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Инструкция: Вы — гуманитарная система для обнаружения нарушений прав человека. Ваша задача — классифицировать, содержит ли публикация в социальных сетях упоминания о нарушениях прав человека.\n",
        "\n",
        "  Что считается нарушением прав человека:\n",
        "  - Убийство мирных жителей\n",
        "  - Уничтожение гражданских объектов\n",
        "  - Изнасилование, пытки, казни\n",
        "\n",
        "  Пример 1:\n",
        "  Пост: В Николаеве, после прилёта в дом, уже пятеро погибших...\n",
        "  Лейбл: Да\n",
        "\n",
        "  Пример 2:\n",
        "  Пост: Российская авиация нанесла удар по бассейну «Нептун» в Мариуполе, где находились беременные женщины и матери с детьми до 3 лет. Орки, воюющие против мирных жителей, — для вас в аду приготовлен особый котел. Информатор | Подписывайтесь.\n",
        "  Лейбл: Да\n",
        "\n",
        "  Пример 3:\n",
        "  Пост: Первый эпизод сыгран. Будут другие. И еще. Я выражу личную позицию. Не могу не упомянуть об этом сейчас. Украинское государство в его нынешней конфигурации с нацистским политическим режимом будет представлять постоянную, прямую и явную угрозу для России. Поэтому, помимо защиты нашего народа и охраны границ страны, целью наших будущих действий, по моему мнению, должно стать полное демонтаж политического режима в Украине.\n",
        "  Лейбл: Нет\n",
        "\n",
        "  Теперь классифицируйте следующий пост:\n",
        "  Пост: \"{text}\"\n",
        "\n",
        "  Ответьте только \"Да\" или \"Нет\":\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "  # Generate label using LLaMA\n",
        "  response = llama_pipeline(prompt, truncation=True)\n",
        "  label = response[0]['generated_text'].strip().lower()\n",
        "  #return label\n",
        "\n",
        "  # Normalize and classify the output\n",
        "  if 'yes' in label or 'да' in label or 'hrv' in label or 'Yes' in label:\n",
        "      return 'Yes'\n",
        "  else:\n",
        "      return 'No'\n",
        "\n",
        "\n",
        "# Annotate the dataset using LLaMA\n",
        "annotations = [one_shot_annotate(text) for text in data]\n",
        "\n",
        "# Save annotations to a DataFrame\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': annotations,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000samples.csv')\n",
        "\n",
        "# Evaluation against ground truth\n",
        "#human_labels = dataset['HRV label'].tolist()\n",
        "llama_labels = annotations_df['annotation'].tolist()\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(human_labels, llama_labels)\n",
        "precision = precision_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "recall = recall_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "f1 = f1_score(human_labels, llama_labels, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Model: \", name)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "#results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GPT4o"
      ],
      "metadata": {
        "id": "8a0bjkJjtNYo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1hVDazUitNYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOI4DN1atNYo"
      },
      "source": [
        "####GPT 4o Few Shot, eng prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b09b5c-c0e0-4e95-8681-a5fbe22e2f78",
        "id": "--sN31aotNYo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  GPT_4o_Few_Shot_eng_prompt\n",
            "Sample size:  1000\n",
            "Accuracy: 0.6793478260869565\n",
            "Precision: 0.6533333333333333\n",
            "Recall: 0.5975609756097561\n",
            "F1 Score: 0.6242038216560509\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_agreement_set.txt\n",
            "time: 1min 16s (started: 2024-12-09 23:23:55 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#  GPT 4o Few Shot Learning -- ENG instructions, WITH EXAMPLES\n",
        "\n",
        "# started with $11.76, ended with $10.58, took 7min 16s\n",
        "\n",
        "import openai\n",
        "\n",
        "name = 'GPT_4o_Few_Shot_eng_prompt'\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "def label_with_chatgpt(prompt, data):\n",
        "    responses = []\n",
        "    for item in data:\n",
        "        full_prompt = f\"{prompt}\\n\\nData: {item}\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "            max_tokens=50\n",
        "        )\n",
        "        label = response.choices[0].message['content'].strip()\n",
        "        responses.append(label)\n",
        "    return responses\n",
        "\n",
        "\n",
        "### ALL ENGLIGH PROMPT\n",
        "prompt = \"\"\"Instruction: You are a humanitarian state-of-the-art Human Rights Violation (HRV) detection system. Given a social media post, your job is to classify\n",
        "if the post contains HRV mentions. HRV includes (but not limited to): 1) Killing civilians, 2) Destruction of civil objects, 3) Rape, torture, executions. You need to return just either \"Yes\" or \"No\" label.\n",
        "Here are some examples:\n",
        "\n",
        "Example 1\n",
        "\n",
        "Post: In Mykolaiv, after the arrival at the house, there are already five dead…\n",
        "Label: Yes\n",
        "\n",
        "Example 2\n",
        "\n",
        "Post: Russian aviation struck the Neptune pool in Mariupol where there were pregnant women and mothers with children under 3 years old. Orcs fighting against civilians - a special cauldron in hell is prepared for you. Informer | Subscribe\n",
        "Label: Yes\n",
        "\n",
        "Example 3\n",
        "\n",
        "Post: The first episode is played. There will be others. And more. I will express a personal position. I cannot fail to mention this now. The Ukrainian state in its current configuration with a Nazi political regime will pose a constant, direct, and clear threat to Russia. Therefore, in addition to protecting our people and guarding the country's borders, the goal of our future actions, in my opinion, should be the full dismantling of the political regime in Ukraine.\n",
        "Label: No\n",
        "\n",
        "Note that the destruction of military objects (for example, an airbase) is not considered HRV.\n",
        "\n",
        "You need to putput label only.\"\"\"\n",
        "\n",
        "\n",
        "#data = data['Post'].to_list()\n",
        "\n",
        "# Get labels\n",
        "labels_eng_prompt = label_with_chatgpt(prompt, data)\n",
        "\n",
        "def normalize_label(labels):\n",
        "  out = []\n",
        "  for label in labels:\n",
        "    label = label.lower()  # Normalize to lowercase\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label:  # Check for relevant terms\n",
        "      out.append('Yes')\n",
        "    else:\n",
        "      out.append('No')\n",
        "  return out\n",
        "\n",
        "# Apply the normalization function to the list\n",
        "labels_eng_prompt = normalize_label(labels_eng_prompt)\n",
        "\n",
        "labels_eng_prompt = pd.DataFrame(labels_eng_prompt)\n",
        "\n",
        "#labels_eng_prompt.to_csv('/content/gdrive/My Drive/HRV NLP Project X/Data/Data [January 2024]/gpt_labels_eng_prompt_hrv.csv')\n",
        "\n",
        "\n",
        "# evaluation\n",
        "\n",
        "# encoding\n",
        "#labels_eng_prompt['labels'] = labels_eng_prompt[0].replace({'Yes': 1, 'No': 0})\n",
        "gpt_labels = labels_eng_prompt[0].to_list()\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_true = human_labels\n",
        "y_pred = gpt_labels\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "print(\"Model: \", name)\n",
        "print(\"Sample size: \", n)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "recall = recall_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "f1 = f1_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': gpt_labels,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "#results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR1Ofq9StNYo"
      },
      "source": [
        "####GPT 4o Few Shot, rus prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa31556-f76d-4c1c-b584-ba12c8b55db3",
        "id": "hOSCUDnPtNYo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  GPT_4o_Few_Shot_rus_prompt\n",
            "Sample size:  184\n",
            "Accuracy: 0.6739130434782609\n",
            "Precision: 0.6486486486486487\n",
            "Recall: 0.5853658536585366\n",
            "F1 Score: 0.6153846153846154\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_agreement_set.txt\n",
            "time: 1min 18s (started: 2024-12-09 23:26:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#  GPT 4o Few Shot Learning -- ENG instructions, WITH EXAMPLES\n",
        "\n",
        "# started with $10.58, ended with $9.28, took 5min 40s\n",
        "\n",
        "import openai\n",
        "\n",
        "name = 'GPT_4o_Few_Shot_rus_prompt'\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "def label_with_chatgpt(prompt, data):\n",
        "    responses = []\n",
        "    for item in data:\n",
        "        full_prompt = f\"{prompt}\\n\\nData: {item}\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "            max_tokens=50\n",
        "        )\n",
        "        label = response.choices[0].message['content'].strip()\n",
        "        responses.append(label)\n",
        "    return responses\n",
        "\n",
        "\n",
        "### ALL RUSSIAN PROMPT\n",
        "\n",
        "prompt = \"\"\"Инструкция: Вы — современная гуманитарная онлайн система для обнаружения нарушений прав человека. Получив публикацию в социальных сетях, ваша задача — классифицировать,\n",
        "содержит ли она упоминания о нарушениях прав человека. Нарушения включают (но не ограничиваются): 1) Убийство мирных жителей, 2) Уничтожение гражданских объектов, 3) Изнасилование, пытки, казни.\n",
        "Вы должны возвращать только лейбл \"Да\" или \"Нет\".\n",
        "\n",
        "Примеры:\n",
        "\n",
        "Пример 1\n",
        "\n",
        "Пост: В Николаеве, после прилёта в дом, уже пятеро погибших...\n",
        "лейбл: Да\n",
        "\n",
        "Пример 2\n",
        "\n",
        "Пост: Российская авиация нанесла удар по бассейну «Нептун» в Мариуполе, где находились беременные женщины и матери с детьми до 3 лет. Орки, воюющие против мирных жителей, — для вас в аду приготовлен особый котел. Информатор | Подписывайтесь.\n",
        "лейбл: Да\n",
        "\n",
        "Пример 3\n",
        "\n",
        "Пост: Первый эпизод сыгран. Будут другие. И еще. Я выражу личную позицию. Не могу не упомянуть об этом сейчас. Украинское государство в его нынешней конфигурации с нацистским политическим режимом будет представлять постоянную, прямую и явную угрозу для России. Поэтому, помимо защиты нашего народа и охраны границ страны, целью наших будущих действий, по моему мнению, должно стать полное демонтаж политического режима в Украине.\n",
        "лейбл: Нет\n",
        "\n",
        "Учтите, что уничтожение военных объектов (например, авиабазы) не считается нарушением прав человека.\n",
        "\n",
        "Вы должны предоставить только лейбл.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Get labels\n",
        "labels_eng_prompt = label_with_chatgpt(prompt, data)\n",
        "\n",
        "def normalize_label(labels):\n",
        "  out = []\n",
        "  for label in labels:\n",
        "    label = label.lower()  # Normalize to lowercase\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label:  # Check for relevant terms\n",
        "      out.append('Yes')\n",
        "    else:\n",
        "      out.append('No')\n",
        "  return out\n",
        "\n",
        "# Apply the normalization function to the list\n",
        "labels_eng_prompt = normalize_label(labels_eng_prompt)\n",
        "\n",
        "labels_eng_prompt = pd.DataFrame(labels_eng_prompt)\n",
        "\n",
        "#labels_eng_prompt.to_csv('/content/gdrive/My Drive/HRV NLP Project X/Data/Data [January 2024]/gpt_labels_eng_prompt_hrv.csv')\n",
        "\n",
        "\n",
        "# evaluation\n",
        "\n",
        "# encoding\n",
        "#labels_eng_prompt['labels'] = labels_eng_prompt[0].replace({'Yes': 1, 'No': 0})\n",
        "gpt_labels = labels_eng_prompt[0].to_list()\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_true = human_labels\n",
        "y_pred = gpt_labels\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "print(\"Model: \", name)\n",
        "print(\"Sample size: \", len(data))\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "recall = recall_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "f1 = f1_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': gpt_labels,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "#results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {len(data)}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kUWSACrtNYo"
      },
      "source": [
        "####GPT 4o One Shot, eng prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a23f9c5-5b40-436e-c4e7-8c2a523ccfef",
        "id": "zQtS9SDgtNYo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  GPT_4o_One_Shot_eng_prompt\n",
            "Sample size:  1000\n",
            "Accuracy: 0.6195652173913043\n",
            "Precision: 0.55\n",
            "Recall: 0.8048780487804879\n",
            "F1 Score: 0.6534653465346535\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_agreement_set.txt\n",
            "time: 1min 23s (started: 2024-12-09 23:29:21 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#  GPT 4o One Shot Learning -- ENG instructions, NO EXAMPLES\n",
        "\n",
        "# started with $9.28, ended with $8.72\n",
        "import openai\n",
        "\n",
        "name = 'GPT_4o_One_Shot_eng_prompt'\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "def label_with_chatgpt(prompt, data):\n",
        "    responses = []\n",
        "    for item in data:\n",
        "        full_prompt = f\"{prompt}\\n\\nData: {item}\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "            max_tokens=50\n",
        "        )\n",
        "        label = response.choices[0].message['content'].strip()\n",
        "        responses.append(label)\n",
        "    return responses\n",
        "\n",
        "\n",
        "### ALL ENGLIGH PROMPT\n",
        "prompt = \"\"\"Instruction: You are a humanitarian state-of-the-art Human Rights Violation (HRV) detection system. Given a social media post, your job is to classify\n",
        "if the post contains HRV mentions. HRV includes (but not limited to): 1) Killing civilians, 2) Destruction of civil objects, 3) Rape, torture, executions. You need to return just either \"Yes\" or \"No\" label. \"\"\"\n",
        "\n",
        "\n",
        "# Get labels\n",
        "labels_eng_prompt = label_with_chatgpt(prompt, data)\n",
        "\n",
        "def normalize_label(labels):\n",
        "  out = []\n",
        "  for label in labels:\n",
        "    label = label.lower()  # Normalize to lowercase\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label:  # Check for relevant terms\n",
        "      out.append('Yes')\n",
        "    else:\n",
        "      out.append('No')\n",
        "  return out\n",
        "\n",
        "# Apply the normalization function to the list\n",
        "labels_eng_prompt = normalize_label(labels_eng_prompt)\n",
        "\n",
        "labels_eng_prompt = pd.DataFrame(labels_eng_prompt)\n",
        "\n",
        "#labels_eng_prompt.to_csv('/content/gdrive/My Drive/HRV NLP Project X/Data/Data [January 2024]/gpt_labels_eng_prompt_hrv.csv')\n",
        "\n",
        "\n",
        "# evaluation\n",
        "\n",
        "# encoding\n",
        "#labels_eng_prompt['labels'] = labels_eng_prompt[0].replace({'Yes': 1, 'No': 0})\n",
        "gpt_labels = labels_eng_prompt[0].to_list()\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_true = human_labels\n",
        "y_pred = gpt_labels\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "print(\"Model: \", name)\n",
        "print(\"Sample size: \", n)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "recall = recall_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "f1 = f1_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': gpt_labels,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "#results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb6RDphctNYo"
      },
      "source": [
        "####GPT 4o One Shot, rus prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "847428c8-4ddf-441f-a3b2-63656cf5c173",
        "id": "KMiasRuHtNYo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  GPT_4o_One_Shot_rus_prompt\n",
            "Sample size:  1000\n",
            "Accuracy: 0.8419117647058824\n",
            "Precision: 0.8200836820083682\n",
            "Recall: 0.9011494252873563\n",
            "F1 Score: 0.8587075575027382\n",
            "Results written to /content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_agreement_set.txt\n",
            "time: 7min 35s (started: 2024-12-09 23:06:24 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#  GPT 4o One Shot Learning -- ENG instructions, NO EXAMPLES\n",
        "\n",
        "# started with $8.72, ended with $8.10, took 8min 27s\n",
        "import openai\n",
        "\n",
        "name = 'GPT_4o_One_Shot_rus_prompt'\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "def label_with_chatgpt(prompt, data):\n",
        "    responses = []\n",
        "    for item in data:\n",
        "        full_prompt = f\"{prompt}\\n\\nData: {item}\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "            max_tokens=50\n",
        "        )\n",
        "        label = response.choices[0].message['content'].strip()\n",
        "        responses.append(label)\n",
        "    return responses\n",
        "\n",
        "\n",
        "### ALL RUSSIAN PROMPT\n",
        "prompt = \"\"\"Инструкция: Вы — современная гуманитарная онлайн система для обнаружения нарушений прав человека.\n",
        "Получив публикацию в социальных сетях, ваша задача — классифицировать, содержит ли она упоминания о нарушениях прав человека.\n",
        "Нарушения прав человека включают (но не ограничиваются): 1) Убийство мирных жителей, 2) Уничтожение гражданских объектов,\n",
        "3) Изнасилование, пытки, казни. Вы должны возвращать только лейбл \"Да\" или \"Нет\" \"\"\"\n",
        "\n",
        "\n",
        "# Get labels\n",
        "labels_eng_prompt = label_with_chatgpt(prompt, data)\n",
        "\n",
        "def normalize_label(labels):\n",
        "  out = []\n",
        "  for label in labels:\n",
        "    label = label.lower()  # Normalize to lowercase\n",
        "    if 'yes' in label or 'да' in label or 'hrv' in label:  # Check for relevant terms\n",
        "      out.append('Yes')\n",
        "    else:\n",
        "      out.append('No')\n",
        "  return out\n",
        "\n",
        "# Apply the normalization function to the list\n",
        "labels_eng_prompt = normalize_label(labels_eng_prompt)\n",
        "\n",
        "labels_eng_prompt = pd.DataFrame(labels_eng_prompt)\n",
        "\n",
        "# evaluation\n",
        "\n",
        "# encoding\n",
        "#labels_eng_prompt['labels'] = labels_eng_prompt[0].replace({'Yes': 1, 'No': 0})\n",
        "gpt_labels = labels_eng_prompt[0].to_list()\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_true = human_labels\n",
        "y_pred = gpt_labels\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "print(\"Model: \", name)\n",
        "print(\"Sample size: \", n)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "recall = recall_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "f1 = f1_score(y_true, y_pred, pos_label=\"Yes\")\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "annotations_df = pd.DataFrame({\n",
        "    'text': data,\n",
        "    'annotation': gpt_labels,\n",
        "    'human label': human_labels\n",
        "})\n",
        "annotations_df.to_csv('/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels/'+ name +'1000.csv')\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "#results_file = \"/content/gdrive/MyDrive/RSRCH 2025/LLM Annotation Analysis/experiment_results_1000samples.txt\"\n",
        "\n",
        "# Append results to the text file\n",
        "with open(results_file, \"a\") as file:\n",
        "    file.write(f\"Date and Time: {current_datetime}\\n\")\n",
        "    file.write(f\"Model: {name}\\n\")\n",
        "    file.write(f\"Samples: {n}\\n\")\n",
        "    file.write(f\"Prompt: {prompt}\\n\")\n",
        "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
        "    file.write(f\"Precision: {precision}\\n\")\n",
        "    file.write(f\"Recall: {recall}\\n\")\n",
        "    file.write(f\"F1 Score: {f1}\\n\")\n",
        "    file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Results written to {results_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62p7LH8ifN-r"
      },
      "source": [
        "### **ERRORS EVALUTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R5W1ajEfVI7",
        "outputId": "5e5fa16a-93fc-4e45-e592-ecefd90e1c39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Combined DataFrame created successfully!\n",
            "                                                text human label  \\\n",
            "0  **12 погибших в результате удара по Запорожью*...         Yes   \n",
            "1  **29 сентября в окрестностях Кременной был лик...         Yes   \n",
            "2  **БЕЗ РОССИИ - НИКУДА:**\\n\"__Два прилета было,...         Yes   \n",
            "3  **В \"демократической\" Латвии\\n\\n**__Сидящий в ...         Yes   \n",
            "4  **В Истре в коттеджном посёлке «Шервуд» мужчин...          No   \n",
            "\n",
            "  GPT_3.5_One_Shot_eng_prompt Mistral7B_One_shot_rus_prompt  \\\n",
            "0                         Yes                            No   \n",
            "1                         Yes                           Yes   \n",
            "2                         Yes                            No   \n",
            "3                         Yes                           Yes   \n",
            "4                         Yes                           Yes   \n",
            "\n",
            "  Claude2_One_Shot_eng_prompt Claude2_One_Shot_rus_prompt  \\\n",
            "0                         Yes                         Yes   \n",
            "1                          No                         Yes   \n",
            "2                          No                         Yes   \n",
            "3                         Yes                         Yes   \n",
            "4                          No                         Yes   \n",
            "\n",
            "  Claude2_Few_Shot_rus_prompt Claude2_Few_Shot_eng_prompt  \\\n",
            "0                         Yes                         Yes   \n",
            "1                         Yes                         Yes   \n",
            "2                         Yes                          No   \n",
            "3                         Yes                          No   \n",
            "4                         Yes                          No   \n",
            "\n",
            "  Mistral7B_Few_Shot_eng_prompt GPT_3.5_Few_Shot_eng_prompt  ...  \\\n",
            "0                           Yes                         Yes  ...   \n",
            "1                           Yes                         Yes  ...   \n",
            "2                            No                         Yes  ...   \n",
            "3                           Yes                         Yes  ...   \n",
            "4                            No                          No  ...   \n",
            "\n",
            "  GPT_4o_Few_Shot_eng_prompt GPT_4o_Few_Shot_rus_prompt  \\\n",
            "0                        Yes                        Yes   \n",
            "1                        Yes                        Yes   \n",
            "2                        Yes                        Yes   \n",
            "3                        Yes                        Yes   \n",
            "4                         No                         No   \n",
            "\n",
            "  GPT_4o_One_Shot_eng_prompt GPT_4o_One_Shot_rus_prompt  \\\n",
            "0                        Yes                        Yes   \n",
            "1                        Yes                        Yes   \n",
            "2                        Yes                        Yes   \n",
            "3                        Yes                        Yes   \n",
            "4                         No                         No   \n",
            "\n",
            "  LLama3_One_Shot_eng_prompt Llama3_Few_shot_eng_prompt  \\\n",
            "0                         No                        Yes   \n",
            "1                        Yes                         No   \n",
            "2                         No                        Yes   \n",
            "3                         No                        Yes   \n",
            "4                         No                        Yes   \n",
            "\n",
            "  Llama3_Few_shot_rus_prompt Llama3_One_shot_rus_prompt  \\\n",
            "0                        Yes                         No   \n",
            "1                         No                         No   \n",
            "2                        Yes                         No   \n",
            "3                        Yes                         No   \n",
            "4                        Yes                         No   \n",
            "\n",
            "  Mistral7B_Few_Shot_rus_prompt Mistral7B_One_shot_eng_prompt  \n",
            "0                           Yes                            No  \n",
            "1                            No                           Yes  \n",
            "2                           Yes                           Yes  \n",
            "3                           Yes                            No  \n",
            "4                           Yes                            No  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "time: 1.21 s (started: 2024-11-24 19:45:45 +00:00)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the folder containing CSV files\n",
        "folder_path = '/content/drive/MyDrive/RSRCH 2025/LLM Annotation Analysis/Labels'\n",
        "\n",
        "# Initialize the combined DataFrame as None\n",
        "combined_df = None\n",
        "\n",
        "# Iterate over files in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.csv'):  # Check if the file is a CSV\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        # Remove the .csv extension for the column name\n",
        "        df_name = os.path.splitext(file_name)[0]\n",
        "        # Read the CSV file into a DataFrame\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Check if required columns exist in the CSV\n",
        "        if not all(col in df.columns for col in ['text', 'human label', 'annotation']):\n",
        "            print(f\"Skipping {file_name} as it does not have the required columns.\")\n",
        "            continue\n",
        "\n",
        "        # Retain only the required columns\n",
        "        df = df[['text', 'human label', 'annotation']]\n",
        "        # Rename the 'annotation' column to the file-specific column name\n",
        "        df = df.rename(columns={'annotation': df_name})\n",
        "\n",
        "        # Merge with the combined DataFrame\n",
        "        if combined_df is None:\n",
        "            combined_df = df  # Initialize with the first file\n",
        "        else:\n",
        "            combined_df = pd.merge(\n",
        "                combined_df,\n",
        "                df,\n",
        "                on=['text', 'human label'],\n",
        "                how='outer'\n",
        "            )\n",
        "\n",
        "# Display the combined DataFrame\n",
        "if combined_df is not None:\n",
        "    print(\"Combined DataFrame created successfully!\")\n",
        "    print(combined_df.head())\n",
        "else:\n",
        "    print(\"No valid CSV files found.\")\n",
        "\n",
        "# Save the combined DataFrame to a CSV if needed\n",
        "combined_df.to_csv('/content/drive/MyDrive/RSRCH 2025/LLM Annotation Analysis/combined_labeld.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "g4vsY0M9jjPH",
        "outputId": "128570e0-22e7-456c-b335-2f6cd689e32b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "combined_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-c35f2955-3e4f-499b-a15c-00e4f57cae79\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>human label</th>\n",
              "      <th>GPT_3.5_One_Shot_eng_prompt</th>\n",
              "      <th>Mistral7B_One_shot_rus_prompt</th>\n",
              "      <th>Claude2_One_Shot_eng_prompt</th>\n",
              "      <th>Claude2_One_Shot_rus_prompt</th>\n",
              "      <th>Claude2_Few_Shot_rus_prompt</th>\n",
              "      <th>Claude2_Few_Shot_eng_prompt</th>\n",
              "      <th>Mistral7B_Few_Shot_eng_prompt</th>\n",
              "      <th>GPT_3.5_Few_Shot_eng_prompt</th>\n",
              "      <th>...</th>\n",
              "      <th>GPT_4o_Few_Shot_eng_prompt</th>\n",
              "      <th>GPT_4o_Few_Shot_rus_prompt</th>\n",
              "      <th>GPT_4o_One_Shot_eng_prompt</th>\n",
              "      <th>GPT_4o_One_Shot_rus_prompt</th>\n",
              "      <th>LLama3_One_Shot_eng_prompt</th>\n",
              "      <th>Llama3_Few_shot_eng_prompt</th>\n",
              "      <th>Llama3_Few_shot_rus_prompt</th>\n",
              "      <th>Llama3_One_shot_rus_prompt</th>\n",
              "      <th>Mistral7B_Few_Shot_rus_prompt</th>\n",
              "      <th>Mistral7B_One_shot_eng_prompt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>**12 погибших в результате удара по Запорожью*...</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>...</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>**29 сентября в окрестностях Кременной был лик...</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>...</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>**БЕЗ РОССИИ - НИКУДА:**\\n\"__Два прилета было,...</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>...</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 22 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c35f2955-3e4f-499b-a15c-00e4f57cae79')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c35f2955-3e4f-499b-a15c-00e4f57cae79 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c35f2955-3e4f-499b-a15c-00e4f57cae79');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d132ae15-976d-414f-adb0-690bb0f2ac6a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d132ae15-976d-414f-adb0-690bb0f2ac6a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d132ae15-976d-414f-adb0-690bb0f2ac6a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                text human label  \\\n",
              "0  **12 погибших в результате удара по Запорожью*...         Yes   \n",
              "1  **29 сентября в окрестностях Кременной был лик...         Yes   \n",
              "2  **БЕЗ РОССИИ - НИКУДА:**\\n\"__Два прилета было,...         Yes   \n",
              "\n",
              "  GPT_3.5_One_Shot_eng_prompt Mistral7B_One_shot_rus_prompt  \\\n",
              "0                         Yes                            No   \n",
              "1                         Yes                           Yes   \n",
              "2                         Yes                            No   \n",
              "\n",
              "  Claude2_One_Shot_eng_prompt Claude2_One_Shot_rus_prompt  \\\n",
              "0                         Yes                         Yes   \n",
              "1                          No                         Yes   \n",
              "2                          No                         Yes   \n",
              "\n",
              "  Claude2_Few_Shot_rus_prompt Claude2_Few_Shot_eng_prompt  \\\n",
              "0                         Yes                         Yes   \n",
              "1                         Yes                         Yes   \n",
              "2                         Yes                          No   \n",
              "\n",
              "  Mistral7B_Few_Shot_eng_prompt GPT_3.5_Few_Shot_eng_prompt  ...  \\\n",
              "0                            No                         Yes  ...   \n",
              "1                            No                         Yes  ...   \n",
              "2                            No                         Yes  ...   \n",
              "\n",
              "  GPT_4o_Few_Shot_eng_prompt GPT_4o_Few_Shot_rus_prompt  \\\n",
              "0                        Yes                        Yes   \n",
              "1                        Yes                        Yes   \n",
              "2                        Yes                        Yes   \n",
              "\n",
              "  GPT_4o_One_Shot_eng_prompt GPT_4o_One_Shot_rus_prompt  \\\n",
              "0                        Yes                        Yes   \n",
              "1                        Yes                        Yes   \n",
              "2                        Yes                        Yes   \n",
              "\n",
              "  LLama3_One_Shot_eng_prompt Llama3_Few_shot_eng_prompt  \\\n",
              "0                         No                        Yes   \n",
              "1                        Yes                         No   \n",
              "2                         No                        Yes   \n",
              "\n",
              "  Llama3_Few_shot_rus_prompt Llama3_One_shot_rus_prompt  \\\n",
              "0                        Yes                         No   \n",
              "1                         No                         No   \n",
              "2                        Yes                         No   \n",
              "\n",
              "  Mistral7B_Few_Shot_rus_prompt Mistral7B_One_shot_eng_prompt  \n",
              "0                           Yes                            No  \n",
              "1                            No                           Yes  \n",
              "2                           Yes                           Yes  \n",
              "\n",
              "[3 rows x 22 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 16 ms (started: 2024-11-24 19:08:45 +00:00)\n"
          ]
        }
      ],
      "source": [
        "combined_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu-f2PAfk2lT",
        "outputId": "aa362188-720e-429d-83d6-b3a162531647"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 21.2 ms (started: 2024-11-24 19:09:18 +00:00)\n"
          ]
        }
      ],
      "source": [
        "combined_df.to_csv('/content/drive/MyDrive/RSRCH 2025/LLM Annotation Analysis/combined_labels.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bobe8nJPtLXh"
      },
      "source": [
        "##**END HERE**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1lMnp4ssGC4x0Obwmx9UgMm728Tgri6ZY",
      "authorship_tag": "ABX9TyNNEha5nFajdx8asSdA6qVS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0142b9ffd07d4bada8014b1dace686fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_752368fdf48c44089727e1c527567001",
            "placeholder": "​",
            "style": "IPY_MODEL_8a190844e13e498db5b12ebeba0166f6",
            "value": " 2/2 [00:08&lt;00:00,  4.01s/it]"
          }
        },
        "0172deab5f504cadb4ee9e74ee0dcc78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01887a2569c84a7391775724bd8f1e9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "024bc2ee140748d2a25e70198cc9be44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02afe9fb448e4e0fbd2a1b49d51bc5a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca17c43b081e422ab112e20b25abdd38",
            "placeholder": "​",
            "style": "IPY_MODEL_db50ad0abc354a1bb5cb25dfbab74d35",
            "value": " 1.80M/1.80M [00:00&lt;00:00, 30.1MB/s]"
          }
        },
        "0304e380701442748c115abe29b97842": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9577c4968db34825bd737ab42dc11d89",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d21dfc34c8e4cdd954ece284bd5e395",
            "value": 2
          }
        },
        "04184e88470645f88f28f94345b5fe75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04f6f4dc98854784a26cc8e2b14f4487": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0596b96437fc49a19fdf816dc49bbe6e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0736149314fe4ad49719e9ec0228833c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5536298221a491e894c81099eede655",
            "max": 4540516344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94e4335e701a4d31a2068b767380e10b",
            "value": 4540516344
          }
        },
        "0d98fd7fa53f49c89d937db6c817e5ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "102b8928e0c04152ba7788a56595b34f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6002f55035244f8792a2e8f20fbfe8e7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6187b06747f040a0826837e13c696480",
            "value": 2
          }
        },
        "10790ac730654ce28696d55f1eddbd7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1313949c9861425f838d53839a57006a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1421695acb6f4356bb5d10bf9ab3d773": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1448e0c612bb4ec5b8cf2f7f7c047073": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5433afe643724c7ba6042579694513c5",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4f4fc328c4843fe84d16fbd104f8b0e",
            "value": 116
          }
        },
        "14a5e814225b40db8da4c77b75172c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1313949c9861425f838d53839a57006a",
            "placeholder": "​",
            "style": "IPY_MODEL_5d41d35b827141a8b0c4a5b45b78d2a1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "172e4ce5775947c8ba81e0fadf2c93d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19680ba1c75c4eceab061877c18426f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19cf4028950c4fa7b45c005212d23c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ae6dd682c444253a30642d24c4cf715": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27fafdad7aa449c59e0f0785a17606cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_286e58e910c8412f867150703a0d78ba",
            "placeholder": "​",
            "style": "IPY_MODEL_df2aafa28aab46d6bfe889e06fb90a20",
            "value": " 2/2 [00:09&lt;00:00,  4.24s/it]"
          }
        },
        "286e58e910c8412f867150703a0d78ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a4f55ebb08540169ec9fd66722d54aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_589fdd02af3c4be99b49c5934b427398",
              "IPY_MODEL_fec148e84d7e410fb9434b142c81a542",
              "IPY_MODEL_ccc2b8d1b3bb454282ac70b6f6ecea3d"
            ],
            "layout": "IPY_MODEL_fe059c2f8e4b4ab8a6593e764da78a62"
          }
        },
        "2d21dfc34c8e4cdd954ece284bd5e395": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d2a1427da004f74a2e6634711f73f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d41eac15929749d9918abf7b60a49a68",
            "max": 25125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d7e0f4f4abe43a2a2a017c97efaa3a5",
            "value": 25125
          }
        },
        "2f0fa77149764fcb8aa0c5d0b0299fb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_471206904bd644b5a6c2efd6092d31d2",
            "placeholder": "​",
            "style": "IPY_MODEL_820a33cf15244ce48d411ddf54934b98",
            "value": "generation_config.json: 100%"
          }
        },
        "2f14782213734dbfab090e3bd0fe6474": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30492072c4424fd19f8b2038a3126770": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4655b54677b8496db656fd45892fff35",
            "placeholder": "​",
            "style": "IPY_MODEL_1421695acb6f4356bb5d10bf9ab3d773",
            "value": " 4.54G/4.54G [00:22&lt;00:00, 207MB/s]"
          }
        },
        "30856fa621654a4390dbca5d171343e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3467d096bbf44bd2bf9332a7282e61df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "351d1e1af31944aa91b107fa9f9ad396": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3662ca4ba4834d98b8a0747e49ff420e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39b7c8293d2c492cb4a91a31381ef5e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_421bfc60c6d24382abe8d85a58c94f0b",
              "IPY_MODEL_0736149314fe4ad49719e9ec0228833c",
              "IPY_MODEL_30492072c4424fd19f8b2038a3126770"
            ],
            "layout": "IPY_MODEL_b750ba7ffc47446b9f2fbcb6fa9a34bf"
          }
        },
        "39ddd4e394ce40cf833aee7848bf348d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a54afe385b8c4c73a1a26496d219f14d",
            "placeholder": "​",
            "style": "IPY_MODEL_404861031aa8455b985842f8d6943a90",
            "value": " 116/116 [00:00&lt;00:00, 9.88kB/s]"
          }
        },
        "3a94acf08e0646e39b53aef79abf6428": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8ef6ac453ef451380b80c04e4b9f8a8",
            "placeholder": "​",
            "style": "IPY_MODEL_04f6f4dc98854784a26cc8e2b14f4487",
            "value": " 571/571 [00:00&lt;00:00, 50.4kB/s]"
          }
        },
        "3d48f6c073e94b438d3bac174f7fc1de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fcfc2e45d8845fa93d8252958264a48",
            "max": 493443,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_debe8411da6a4587a7b4b2f7678e5f57",
            "value": 493443
          }
        },
        "404861031aa8455b985842f8d6943a90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "421bfc60c6d24382abe8d85a58c94f0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0596b96437fc49a19fdf816dc49bbe6e",
            "placeholder": "​",
            "style": "IPY_MODEL_ecfe9dbec0e6447781220bb32d47b107",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "4325c36cb18848748954e7ed14bbd523": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4513d2be315d4463be76ab919ce46b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b2efd41b3fd4121b7a83e5380738ea0",
            "placeholder": "​",
            "style": "IPY_MODEL_84ab83fa39184b9cb62f8d587055fdcd",
            "value": "config.json: 100%"
          }
        },
        "4655b54677b8496db656fd45892fff35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4690426fca3d4de89684ae0d8325b2de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f0fa77149764fcb8aa0c5d0b0299fb8",
              "IPY_MODEL_1448e0c612bb4ec5b8cf2f7f7c047073",
              "IPY_MODEL_39ddd4e394ce40cf833aee7848bf348d"
            ],
            "layout": "IPY_MODEL_9e10195d3d384eb5a524d1bf91443adb"
          }
        },
        "471206904bd644b5a6c2efd6092d31d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49f888ec343643189a98c15d8697a3f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d5192b4eb3c431aab4affc10ab973cc",
              "IPY_MODEL_acfed305e83747d6ae42de6c9d978f4b",
              "IPY_MODEL_a6cb7b8478cd4a9ba4c9d0cb7f808bf9"
            ],
            "layout": "IPY_MODEL_830a82825b4449049fd2db141103de97"
          }
        },
        "4d7e0f4f4abe43a2a2a017c97efaa3a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50895209ff9f402cbd4cd73c54a0c6a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77287599114d45bba367ca594196d624",
            "placeholder": "​",
            "style": "IPY_MODEL_0d98fd7fa53f49c89d937db6c817e5ca",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "508f59c5748646888fb90db7bdfe3b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_615adb06c7644863b725561c0a388062",
            "placeholder": "​",
            "style": "IPY_MODEL_5cf33c9480094ccea0c9516707309436",
            "value": "tokenizer.model: 100%"
          }
        },
        "50b36f786dea4343aebec9aea89784e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "516b28a3539b4983ad683da272de1e3f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5433afe643724c7ba6042579694513c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54db2a7091f24dc98cc697e73d1c0335": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a23d8276924644629a85fe2d9c74a5d6",
              "IPY_MODEL_84e60abe83c7487baee99593f41518cc",
              "IPY_MODEL_27fafdad7aa449c59e0f0785a17606cd"
            ],
            "layout": "IPY_MODEL_5877a5dbf79645fe849b28b9e6b2e604"
          }
        },
        "578a71b25eba48ee84ae5cc8d565c784": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5877a5dbf79645fe849b28b9e6b2e604": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "589fdd02af3c4be99b49c5934b427398": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a9b6faff37d4c8582eecc92e38d5c0c",
            "placeholder": "​",
            "style": "IPY_MODEL_1ae6dd682c444253a30642d24c4cf715",
            "value": "Downloading shards: 100%"
          }
        },
        "5930e72a6f194a5aa1d1a7848e0d3792": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5cf33c9480094ccea0c9516707309436": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d41d35b827141a8b0c4a5b45b78d2a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f45333adc004573bba298f0266860d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6002f55035244f8792a2e8f20fbfe8e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "608672f0b1fc4c5cb17e9ffb3328a798": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "615adb06c7644863b725561c0a388062": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6187b06747f040a0826837e13c696480": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "644a97d1cdbf4a29baf98a1a0c19de84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66316e15798f4bc896a6a94b00ccd8e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66b4040f2c0843158efdb13f5b34836b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6883044661504a40a083eb48dc9f9bda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6888de350abf43679bcd676939637832": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3467d096bbf44bd2bf9332a7282e61df",
            "placeholder": "​",
            "style": "IPY_MODEL_87d1ef463cc74256bbf2211d085f6a29",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6a9b6faff37d4c8582eecc92e38d5c0c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b2efd41b3fd4121b7a83e5380738ea0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d5192b4eb3c431aab4affc10ab973cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66b4040f2c0843158efdb13f5b34836b",
            "placeholder": "​",
            "style": "IPY_MODEL_d256260559cd41739c6dfa4dd9f56c2f",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "739e93c24ade4569929d83d6bcad53f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75000ccd3a344cf4b1dfd930f2598a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "752368fdf48c44089727e1c527567001": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75cf74fc723246c79ad58959107f6f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77287599114d45bba367ca594196d624": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b51fa5e90aa47658614a89698674551": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dbce6f459914ecc9354cd5b49ade08e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7de447c562ee425997f3ea6089a8e143": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89ac018898a04cd1b5a75e1d81e8c157",
              "IPY_MODEL_c55326a1e7b440488ed616d9764ec0e8",
              "IPY_MODEL_a8b5536876564b34a0d2adbabfa37f6b"
            ],
            "layout": "IPY_MODEL_578a71b25eba48ee84ae5cc8d565c784"
          }
        },
        "7e3eb7b9b97841b8ba7c4c18ef8a8b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fcfc2e45d8845fa93d8252958264a48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "820a33cf15244ce48d411ddf54934b98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "830a82825b4449049fd2db141103de97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8321cfbaa21c43949148a1b68d05549f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dbce6f459914ecc9354cd5b49ade08e",
            "placeholder": "​",
            "style": "IPY_MODEL_d0c7005ad0874dbfac6465502baacce2",
            "value": " 493k/493k [00:00&lt;00:00, 35.4MB/s]"
          }
        },
        "84ab83fa39184b9cb62f8d587055fdcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84e60abe83c7487baee99593f41518cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff50c5315f3c43a094cc8c4cfec34950",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19680ba1c75c4eceab061877c18426f2",
            "value": 2
          }
        },
        "87d1ef463cc74256bbf2211d085f6a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "880d343bd4a2480798c1cb437f0eeea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89ac018898a04cd1b5a75e1d81e8c157": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e472f02981e540a093006480c1d0e607",
            "placeholder": "​",
            "style": "IPY_MODEL_a0b9301b9ed74abd9383c72502f35b11",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8a11d475f31f46cc8a8c33da93f0b21b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3f6368eb9b0468f8d26a696493973fe",
              "IPY_MODEL_a14fc8cabdc14d3ea9c27023fb09737d",
              "IPY_MODEL_ab0777654949491eaf044c11376b3d91"
            ],
            "layout": "IPY_MODEL_e08d1db672704ccba9eee86b1006adf0"
          }
        },
        "8a190844e13e498db5b12ebeba0166f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ab66c1fc67c43419a79e6b510f0c211": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c5dfdce347741aa811d7cdd683d53cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f66bc6e840f45e5925b973fc51f644f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9015717d6d1b4a08ba526748ead729e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9142f180074645fa99f78d56e36f0ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "916935281df84b8793bda53e37556147": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0172deab5f504cadb4ee9e74ee0dcc78",
            "max": 1795188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_608672f0b1fc4c5cb17e9ffb3328a798",
            "value": 1795188
          }
        },
        "92856b9799144623bd58b3e1d52fa3ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2b579d31b914204a30f757eeb4d42c1",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5930e72a6f194a5aa1d1a7848e0d3792",
            "value": 571
          }
        },
        "94e4335e701a4d31a2068b767380e10b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9577c4968db34825bd737ab42dc11d89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9580e5325d9c426b905413bc3afc44cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95d409bcc5f947afa37ef49d28be6a5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f45333adc004573bba298f0266860d7",
            "placeholder": "​",
            "style": "IPY_MODEL_04184e88470645f88f28f94345b5fe75",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "9ad982b224f34de1831455021caee99b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b423df2e924403eb6b4938f1864aa54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_172e4ce5775947c8ba81e0fadf2c93d3",
            "placeholder": "​",
            "style": "IPY_MODEL_19cf4028950c4fa7b45c005212d23c23",
            "value": " 414/414 [00:00&lt;00:00, 34.6kB/s]"
          }
        },
        "9db9262dcec64dda892d8393e8ca403c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3e1803fd9cc4086af0670ba3abe1ff4",
            "placeholder": "​",
            "style": "IPY_MODEL_c5b9f1c971db45e8aecd85aa3d133e7a",
            "value": "tokenizer.json: 100%"
          }
        },
        "9e0cca83666d4543829dac3b83b5d088": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e10195d3d384eb5a524d1bf91443adb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0b9301b9ed74abd9383c72502f35b11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a14ec6a9cbc340b4879b7d84038db346": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f66bc6e840f45e5925b973fc51f644f",
            "placeholder": "​",
            "style": "IPY_MODEL_c44cc0dde60b4486a9473dbe82a7dd97",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "a14fc8cabdc14d3ea9c27023fb09737d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50b36f786dea4343aebec9aea89784e0",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_644a97d1cdbf4a29baf98a1a0c19de84",
            "value": 2
          }
        },
        "a23d8276924644629a85fe2d9c74a5d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b84edcef9ed547e7b69f5c36bc8aeff2",
            "placeholder": "​",
            "style": "IPY_MODEL_b4a8caea1d9941f5ae09900b2913c7ea",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a2f22f03f9ff411fa3a86f636aaf3689": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a31b01033c394317b0762a519e006a09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2f22f03f9ff411fa3a86f636aaf3689",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6883044661504a40a083eb48dc9f9bda",
            "value": 2
          }
        },
        "a3e1803fd9cc4086af0670ba3abe1ff4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a51c9a610682423eb5d24b7a9f418f50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a54afe385b8c4c73a1a26496d219f14d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6cb7b8478cd4a9ba4c9d0cb7f808bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66316e15798f4bc896a6a94b00ccd8e5",
            "placeholder": "​",
            "style": "IPY_MODEL_75000ccd3a344cf4b1dfd930f2598a83",
            "value": " 9.94G/9.94G [00:47&lt;00:00, 212MB/s]"
          }
        },
        "a8b5536876564b34a0d2adbabfa37f6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_024bc2ee140748d2a25e70198cc9be44",
            "placeholder": "​",
            "style": "IPY_MODEL_9142f180074645fa99f78d56e36f0ebf",
            "value": " 2/2 [00:08&lt;00:00,  3.86s/it]"
          }
        },
        "ab0777654949491eaf044c11376b3d91": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8a0a2c338d14e6e87e290d300b8ba9d",
            "placeholder": "​",
            "style": "IPY_MODEL_f24b7a9c8f1f47c39dee6a5eed928fcf",
            "value": " 2/2 [00:08&lt;00:00,  3.89s/it]"
          }
        },
        "ab1d696b35d64a7ca24ef37c3bcded51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac127432b0124ccf9975dcb96d25a955": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acfed305e83747d6ae42de6c9d978f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f16743eef90d4e2a969dfa5517fa2c29",
            "max": 9942981696,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_880d343bd4a2480798c1cb437f0eeea4",
            "value": 9942981696
          }
        },
        "adce3e35f65f4ad7a19d3aca9bea617d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_739e93c24ade4569929d83d6bcad53f8",
            "placeholder": "​",
            "style": "IPY_MODEL_8ab66c1fc67c43419a79e6b510f0c211",
            "value": " 2/2 [00:07&lt;00:00,  3.70s/it]"
          }
        },
        "b4362d6b909f4ab9b1c9929cd325c71d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9946d33406d413eab1e7eaa24847888",
            "placeholder": "​",
            "style": "IPY_MODEL_7e3eb7b9b97841b8ba7c4c18ef8a8b95",
            "value": " 25.1k/25.1k [00:00&lt;00:00, 2.06MB/s]"
          }
        },
        "b45f06fa224d4524af1095ab70072951": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4a8caea1d9941f5ae09900b2913c7ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5536298221a491e894c81099eede655": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b750ba7ffc47446b9f2fbcb6fa9a34bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b84edcef9ed547e7b69f5c36bc8aeff2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba5562c66ab548f1afcdd35c195822b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c33c3c34d5c84ae7906c0f1a57bb60b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50895209ff9f402cbd4cd73c54a0c6a1",
              "IPY_MODEL_0304e380701442748c115abe29b97842",
              "IPY_MODEL_d7af53d2de3f4007b58b99a347925c6a"
            ],
            "layout": "IPY_MODEL_ea26c88ba4b541b0988b2c9f526e7da4"
          }
        },
        "c44cc0dde60b4486a9473dbe82a7dd97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c55326a1e7b440488ed616d9764ec0e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c861810285d64f3c9f182223969bcb9e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30856fa621654a4390dbca5d171343e2",
            "value": 2
          }
        },
        "c5b9f1c971db45e8aecd85aa3d133e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6c49ff0a56043b18b30fc99038f948c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7530c4f59e64d379988a467e87b5f63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14a5e814225b40db8da4c77b75172c5e",
              "IPY_MODEL_a31b01033c394317b0762a519e006a09",
              "IPY_MODEL_adce3e35f65f4ad7a19d3aca9bea617d"
            ],
            "layout": "IPY_MODEL_01887a2569c84a7391775724bd8f1e9b"
          }
        },
        "c813fae22f9548cdbeed247463e01e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6c49ff0a56043b18b30fc99038f948c",
            "placeholder": "​",
            "style": "IPY_MODEL_7b51fa5e90aa47658614a89698674551",
            "value": " 996/996 [00:00&lt;00:00, 80.6kB/s]"
          }
        },
        "c861810285d64f3c9f182223969bcb9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca17c43b081e422ab112e20b25abdd38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc8648e9445c4a33953b589eb6f489c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccc2b8d1b3bb454282ac70b6f6ecea3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ad982b224f34de1831455021caee99b",
            "placeholder": "​",
            "style": "IPY_MODEL_ab1d696b35d64a7ca24ef37c3bcded51",
            "value": " 2/2 [01:10&lt;00:00, 32.94s/it]"
          }
        },
        "d0c7005ad0874dbfac6465502baacce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d19a9b518bff4e8e900588259c18ab8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b45f06fa224d4524af1095ab70072951",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_351d1e1af31944aa91b107fa9f9ad396",
            "value": 414
          }
        },
        "d256260559cd41739c6dfa4dd9f56c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2b579d31b914204a30f757eeb4d42c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d41eac15929749d9918abf7b60a49a68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4f4fc328c4843fe84d16fbd104f8b0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d52e04954e1b4053ab1663f687ca1838": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95d409bcc5f947afa37ef49d28be6a5f",
              "IPY_MODEL_d19a9b518bff4e8e900588259c18ab8d",
              "IPY_MODEL_9b423df2e924403eb6b4938f1864aa54"
            ],
            "layout": "IPY_MODEL_9580e5325d9c426b905413bc3afc44cc"
          }
        },
        "d7af53d2de3f4007b58b99a347925c6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd18e7b7a2074b039d4e8a361693628f",
            "placeholder": "​",
            "style": "IPY_MODEL_8c5dfdce347741aa811d7cdd683d53cb",
            "value": " 2/2 [00:08&lt;00:00,  3.74s/it]"
          }
        },
        "d8d731b22c81460bb1ed9f3d77e7bd23": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db50ad0abc354a1bb5cb25dfbab74d35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd18e7b7a2074b039d4e8a361693628f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd5659f88b68437a878a866eb7332e4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_508f59c5748646888fb90db7bdfe3b0a",
              "IPY_MODEL_3d48f6c073e94b438d3bac174f7fc1de",
              "IPY_MODEL_8321cfbaa21c43949148a1b68d05549f"
            ],
            "layout": "IPY_MODEL_d8d731b22c81460bb1ed9f3d77e7bd23"
          }
        },
        "debe8411da6a4587a7b4b2f7678e5f57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df2aafa28aab46d6bfe889e06fb90a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e08d1db672704ccba9eee86b1006adf0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0e22ff553aa4124bb1ddc9275a5dbd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa34a2c4c8f547d89dc530a9f75e3ae2",
              "IPY_MODEL_efddeff79788444dabf111a036a359f4",
              "IPY_MODEL_c813fae22f9548cdbeed247463e01e1b"
            ],
            "layout": "IPY_MODEL_516b28a3539b4983ad683da272de1e3f"
          }
        },
        "e1a3df38a5024d118463eaeace7bdd2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e472f02981e540a093006480c1d0e607": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9946d33406d413eab1e7eaa24847888": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea26c88ba4b541b0988b2c9f526e7da4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecfe9dbec0e6447781220bb32d47b107": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efddeff79788444dabf111a036a359f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac127432b0124ccf9975dcb96d25a955",
            "max": 996,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba5562c66ab548f1afcdd35c195822b9",
            "value": 996
          }
        },
        "f008e08bafe048828cf0ec6122dc665a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6888de350abf43679bcd676939637832",
              "IPY_MODEL_102b8928e0c04152ba7788a56595b34f",
              "IPY_MODEL_0142b9ffd07d4bada8014b1dace686fa"
            ],
            "layout": "IPY_MODEL_4325c36cb18848748954e7ed14bbd523"
          }
        },
        "f0903d60aa824f87bf1c97e749302cc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a14ec6a9cbc340b4879b7d84038db346",
              "IPY_MODEL_2d2a1427da004f74a2e6634711f73f96",
              "IPY_MODEL_b4362d6b909f4ab9b1c9929cd325c71d"
            ],
            "layout": "IPY_MODEL_e1a3df38a5024d118463eaeace7bdd2e"
          }
        },
        "f16743eef90d4e2a969dfa5517fa2c29": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f24b7a9c8f1f47c39dee6a5eed928fcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3f6368eb9b0468f8d26a696493973fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e0cca83666d4543829dac3b83b5d088",
            "placeholder": "​",
            "style": "IPY_MODEL_a51c9a610682423eb5d24b7a9f418f50",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f8a0a2c338d14e6e87e290d300b8ba9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8ef6ac453ef451380b80c04e4b9f8a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa34a2c4c8f547d89dc530a9f75e3ae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9015717d6d1b4a08ba526748ead729e1",
            "placeholder": "​",
            "style": "IPY_MODEL_2f14782213734dbfab090e3bd0fe6474",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "fb4e8b3cc1bb4d4c8835c3f39d33db54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4513d2be315d4463be76ab919ce46b78",
              "IPY_MODEL_92856b9799144623bd58b3e1d52fa3ae",
              "IPY_MODEL_3a94acf08e0646e39b53aef79abf6428"
            ],
            "layout": "IPY_MODEL_10790ac730654ce28696d55f1eddbd7b"
          }
        },
        "fe059c2f8e4b4ab8a6593e764da78a62": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fec148e84d7e410fb9434b142c81a542": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc8648e9445c4a33953b589eb6f489c4",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75cf74fc723246c79ad58959107f6f89",
            "value": 2
          }
        },
        "ff304bd0ae144e6da3ffe872a5f3b9d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9db9262dcec64dda892d8393e8ca403c",
              "IPY_MODEL_916935281df84b8793bda53e37556147",
              "IPY_MODEL_02afe9fb448e4e0fbd2a1b49d51bc5a7"
            ],
            "layout": "IPY_MODEL_3662ca4ba4834d98b8a0747e49ff420e"
          }
        },
        "ff50c5315f3c43a094cc8c4cfec34950": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}